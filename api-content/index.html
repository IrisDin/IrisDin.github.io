{"posts":[{"title":"# å¥½ä¹¦æ¨è æ— é™è¿‘ä¼¼äºé€æ˜çš„è“","content":" ã€Šæ— é™è¿‘ä¼¼äºé€æ˜çš„è“ã€‹æ˜¯24å²çš„æ‘ä¸Šé¾™ç¬”ä¸‹çš„ç¬¬ä¸€éƒ¨ä½œå“ã€‚æ•…äº‹çš„èƒŒæ™¯å‘ç”Ÿåœ¨äºŒåä¸–çºªä¸ƒåå¹´ä»£å·¦å³ï¼Œé‚£æ—¶çš„æ—¥æœ¬æ­£åœ¨é£é€Ÿçš„å‘å±•ï¼Œç»æµå’Œäººä»¬çš„ç‰©è´¨æ°´å¹³ä¹Ÿå¾—åˆ°äº†å‰æ‰€æœªæœ‰çš„æé«˜ã€‚ä½†åœ¨ä¸œäº¬ç¦ç”ŸåŒºç¾å†›åŸºåœ°é™„è¿‘ï¼Œæœ‰ä¸€ç¾¤æµ‘æµ‘å™©å™©çš„é’å¹´ç”·å¥³æ•´æ—¥çºµæƒ…æ”¾ä»»ï¼Œæ²¡æœ‰çº¦æŸï¼Œå¯¹è‡ªå·±çš„æœªæ¥æ„Ÿåˆ°è¿·èŒ«åˆææƒ§ã€‚ æ–‡ç« å€Ÿç”¨äº†19ç”·å­â€œé¾™â€çš„è§†è§’ï¼Œç›´ç™½çš„å°†æ­»äº¡, æ€§çˆ±, é…’ç²¾ï¼Œè¿·å¹»å‰‚ï¼Œè…çƒ‚çš„é£Ÿç‰©ï¼Œæ¯’å“...æè¿°åœ¨ä¹¦ä¸­ã€‚â€œé¾™â€å°±åƒä½œè€…æ‘ä¸Šé¾™çš„ç¼©å½±ï¼Œåœ¨é»‘æš—çš„æ·±æ¸Šä¸­å°è¯•ç€æ•‘èµè‡ªå·±ï¼Œä½†å´è¶Šé™·è¶Šæ·±ã€‚é¾™æ—¶å¸¸ä¹Ÿåœ¨æ€€ç–‘ï¼Œé‚£ä¸ªç…§é¡¾åŒä¼´ï¼Œçƒ­çˆ±ç¾å¥½é£æ™¯çš„è‡ªå·±æ˜¯ä¸æ˜¯é€æ¸è¢«èº«è¾¹ç³Ÿç³•çš„ç¯å¢ƒæ‰“è´¥äº†ã€‚æœ€åçš„ä»–é€‰æ‹©äº†æ­»äº¡çš„æ–¹å¼æ¥æ•‘èµè‡ªå·±çš„çµé­‚ã€‚é¾™èººåœ¨è‰åœ°ä¸Šï¼Œå¹»æƒ³ç€æ— é™è¿‘ä¼¼äºè“è‰²çš„å¤©ç©ºï¼Œç»“æŸäº†è‡ªå·±çš„ç”Ÿå‘½ã€‚ çœŸå®ç»†èŠ‚çš„æå†™ï¼Œè¿™ç§è‚®è„çš„ç¾å­¦ç”¨æ–‡å­—çš„æ–¹å¼ç»™è¯»è€…å¸¦æ¥å¼ºçƒˆçš„è§†è§‰å†²å‡»å’Œä¸é€‚æ„Ÿã€‚è¿™ç§çœŸè¯šç›´ç™½çš„å†™ä½œæ–¹å¼æ— ç–‘å¯¹å½“æ—¶çš„æ–‡å­¦æµæ´¾å¸¦æ¥äº†å†²å‡»ï¼Œå±•ç°äº†å½“æ—¶ç¤¾ä¼šæœ€çœŸå®çš„é¢“åºŸå’Œæ··æ²Œï¼Œä¹Ÿçœ‹ä¸åˆ°ä¸€ä¸çš„å–œæ‚¦ã€‚ä¹¦ä¸­æç»˜çš„åœºæ™¯å’Œä¸»äººå…¬çš„ç†å¿µæ¯«æ— ç–‘é—®çš„è¯´ä¸ç¬¦åˆä¸€èˆ¬äººä»¬â€œæ­£ç¡®â€çš„ä»·å€¼è§‚ã€‚ è¿™æœ¬å¸¦ç€å¿§éƒè‰²å½©çš„é’æ˜¥æ–‡å­¦åœ¨æ—¥æœ¬æ–‡å­¦ä¸­ç•™ä¸‹äº†è‡ªå·±ç‹¬ç‰¹çš„è‰²å½©ï¼Œå°†äºšæ–‡åŒ–ä¸»é¢˜çš„ä½œå“é€ä¸Šèˆå°ï¼Œå±•ç°äº†é‚£ä¸ªå¹´ä»£â€é€æ˜æ—â€œæœ€çœŸå®çš„ç”Ÿæ´»çŠ¶æ€... ","link":"https://irisdin.github.io/post/BWIR5TCy3/"},{"title":"å¦‚ä½•ç”¨Github pagesè½»æ¾æ­å»ºå±äºè‡ªå·±çš„ä¸ªäººç½‘ç«™ğŸ˜‰","content":"Github pageæ˜¯ä»€ä¹ˆâ“ GitHub Pagesæ˜¯GitHubæä¾›çš„ä¸€ä¸ªç½‘é¡µå¯„å­˜æœåŠ¡ï¼Œå¯ä»¥ç”¨äºå­˜æ”¾é™æ€ç½‘é¡µï¼ŒåŒ…æ‹¬åšå®¢ã€é¡¹ç›®æ–‡æ¡£ï¼Œç”šè‡³æ•´æœ¬ä¹¦ã€‚ä¸€èˆ¬GitHub Pagesçš„ç½‘ç«™ä½¿ç”¨github.ioçš„å­åŸŸåã€‚æœ¬ç«™ç‚¹ https:/irisdin.github.io/ å°±æ˜¯é€šè¿‡ Github pagesè¿›è¡Œæ­å»ºå®Œæˆâœ… ï¼ˆæ¥æºï¼šç»´åŸºç™¾ç§‘ï¼‰ æˆ‘ä¸ºä»€ä¹ˆé€‰æ‹©Github pageâ“ æ— éœ€è´­ä¹°äº‘æœåŠ¡å™¨ å…è´¹ æ²¡æœ‰ç¹ççš„ç¯å¢ƒé…ç½®ä»¥åŠç³»ç»Ÿæ­å»º é€‚åˆé™æ€åšå®¢çš„æ­å»º ä¸è¶³ä¹‹å¤„ï¼šğŸ˜¢ åŸºäºGitï¼Œéœ€è¦äº†è§£ä¸€å®šçš„gitæŒ‡ä»¤å’ŒåŸºç¡€çš„ç¼–ç¨‹çŸ¥è¯† é¡¹ç›®å’Œç½‘ç«™çš„å¤§å°ä¸èƒ½è¶…è¿‡1GB æ¯å°æ—¶ä¸å¾—è¶…è¿‡ 10 ä¸ªæ›´æ–°çš„ç½‘ç«™ç‰ˆæœ¬ æ¯ä¸ªæœˆçš„ä¹Ÿè¦æ³¨æ„å¸¦å®½ä½¿ç”¨ä¸Šé™ä¸º 100GB å¦‚ä½•ä½¿ç”¨Github pages æ¥æ­å»ºè‡ªå·±çš„ç½‘ç«™ æ³¨å†Œä¸€ä¸ªGithubè´¦å· å®˜ç½‘é“¾æ¥ Github åœ¨ä¸ªäººä¸»ç•Œé¢é‡Œæ–°å»ºä¸€ä¸ª Repository åœ¨ new repositoryå¡«å†™åŸŸåï¼ˆä¸å¯ä¿®æ”¹ï¼ï¼‰æ ¼å¼ä¸ºï¼šusername.GitHub.io ä¾‹å¦‚æˆ‘çš„ç”¨æˆ·åæ˜¯IrisDin é‚£åˆ™å¡«å†™IrisDin.github.io åˆ›å»ºå®Œä¹‹åç‚¹è¿›æˆ‘ä»¬çš„repository ç‚¹å‡»setting åœ¨å·¦è¾¹çš„ç›®å½•å½“ä¸­æ‰¾åˆ°pageså¹¶è¿›å…¥Github pagesï¼Œé€‰æ‹©ä¸€ä¸ªè‡ªå·±å–œæ¬¢çš„ä¸»é¢˜ ** æœ€åä¸€æ­¥ é€‰æ‹©å®Œä½ å–œæ¬¢çš„ä¸»é¢˜ä¹‹å GitHub Pages ä¼šè‡ªåŠ¨ç”Ÿæˆç½‘ç«™ã€‚ç‚¹å‡» Commit changes æŒ‰é’®ï¼Œä½ çš„ç½‘ç«™å°±ä¸Šçº¿å•¦ğŸ˜ƒ åœ¨æµè§ˆå™¨é‡Œè¾“å…¥ä½ çš„åŸŸåï¼ˆrepositoryçš„åå­—ï¼‰ï¼Œä¾‹å¦‚ Irisdin.GitHub.ioï¼Œä½ å°±å¯ä»¥çœ‹åˆ°ä½ çš„ç½‘ç«™åœ¨ç½‘é¡µä¸Šçš„æ•ˆæœäº†ã€‚ ä½¿ç”¨ç¬¬ä¸‰æ–¹çš„é™æ€æ¨¡æ¿ç³»ç»Ÿæ¥è‡ªå®šä¹‰è‡ªå·±çš„ç½‘ç«™ githubå®˜æ–¹æä¾›äº†æœ‰é™çš„æ¨¡ç‰ˆï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ç¬¬ä¸‰æ–¹çš„é™æ€æ¨¡ç‰ˆç³»ç»Ÿæ¥ä½¿è‡ªå·±çš„ç½‘ç«™å˜çš„æ›´åŠ ç¾è§‚ï¼Œä¾‹å¦‚ä½¿ç”¨Node.js ç¼–å†™çš„ Hexoï¼ŒGo ç¼–å†™çš„ Hugoå’ŒPython ç¼–å†™çš„ Pelicanã€‚ä¹Ÿå¯ä»¥åœ¨githubä¸­å¤šæµè§ˆæ‰¾åˆ°è‡ªå·±å–œæ¬¢çš„æ¨¡ç‰ˆï¼šï¼‰ å¦‚ä½•é…ç½®è‡ªå®šä¹‰åŸŸå å¦‚æœä½ ä¸å–œæ¬¢github.ioè¿™ä¸ªåç¼€çš„è¯ï¼Œä½ å¯ä»¥è½»æ¾çš„é…ç½®è‡ªå·±å–œæ¬¢çš„åŸŸå Github pageè‡ªå®šä¹‰é…ç½®åŸŸåå®˜æ–¹æ–‡æ¡£ GitHub Pages æä¾›å…è´¹ä¸ºè‡ªå®šä¹‰åŸŸåå¼€å¯ HTTPS çš„åŠŸèƒ½ï¼Œä¸éœ€è¦è‡ªå·±æä¾›è¯ä¹¦ï¼Œåªéœ€è¦å°†è‡ªå·±çš„åŸŸåä½¿ç”¨ CNAME çš„æ–¹å¼æŒ‡å‘è‡ªå·±çš„ GitHub Pages åŸŸåå³å¯ã€‚ï¼ˆæ²¡æœ‰è‡ªå·±çš„åŸŸåçš„è¯å¯ä»¥åœ¨é˜¿é‡Œäº‘æˆ–è€…è…¾è®¯äº‘ä¸Šè´­ä¹°ï¼‰ åœ¨åŸŸåç®¡ç†ä¸­æ·»åŠ ä¸¤æ¡è®°å½•ç±»å‹ä¸ºCNAMEçš„è§£æï¼Œä¸€æ¡ä¸»æœºè®°å½•ä¸º@ï¼Œä¸€æ¡ä¸»æœºè®°å½•ä¸ºwwwï¼Œæ ¼å¼ä¸ºhttp://xxxx.github.io åœ¨æœ¬åœ°hexoæ–‡ä»¶çš„sourceæ–‡ä»¶å¤¹ä¸‹åˆ›å»ºä¸€ä¸ªCNAMEæ–‡ä»¶ï¼Œç¼–è¾‘CNAMEæ–‡ä»¶ï¼Œå¡«å†™åŸŸåï¼Œä¾‹å¦‚http://xxxx.cn åœ¨git page ä¸­è®¾ç½®ä¸­å‹¾é€‰https ç­‰å¾…æ›´æ–°å³å¯ ","link":"https://irisdin.github.io/post/-3Hrfy0Up/"},{"title":"Social networking and recommendation sysytems practice","content":"readme linkğŸ”» uw cse only for practice purpose/plagiarism is not allowedğŸ”» cse misconduct guideline programming language : python import utils # noqa: F401, do not remove if using a Mac import networkx as nx import matplotlib.pyplot as plt from operator import itemgetter &quot;&quot;&quot;Builds and returns the practice graph &quot;&quot;&quot; def get_practice_graph(): practice_graph = nx.Graph() practice_graph.add_node(&quot;A&quot;) practice_graph.add_node(&quot;B&quot;) practice_graph.add_node(&quot;C&quot;) practice_graph.add_node(&quot;D&quot;) practice_graph.add_node(&quot;E&quot;) practice_graph.add_node(&quot;F&quot;) practice_graph.add_edge(&quot;A&quot;, &quot;B&quot;) practice_graph.add_edge(&quot;A&quot;, &quot;C&quot;) practice_graph.add_edge(&quot;B&quot;, &quot;C&quot;) practice_graph.add_edge(&quot;B&quot;, &quot;D&quot;) practice_graph.add_edge(&quot;C&quot;, &quot;D&quot;) practice_graph.add_edge(&quot;C&quot;, &quot;F&quot;) practice_graph.add_edge(&quot;D&quot;, &quot;F&quot;) practice_graph.add_edge(&quot;D&quot;, &quot;E&quot;) return practice_graph def draw_practice_graph(graph): &quot;&quot;&quot;Draw practice_graph to the screen. &quot;&quot;&quot; nx.draw_networkx(graph) plt.show() def get_romeo_and_juliet_graph(): &quot;&quot;&quot;Builds and returns the romeo and juliet graph &quot;&quot;&quot; rj = nx.Graph() rj.add_nodes_from(['Paris', 'Mercutio', 'Escalus', 'Capulet']) rj.add_nodes_from(['Juliet', 'Tybalt', 'Nurse']) rj.add_nodes_from(['Montague', 'Benvolio', 'Romeo', 'Friar Laurence']) rj.add_edge('Romeo', 'Friar Laurence') rj.add_edge('Romeo', 'Benvolio') rj.add_edge('Romeo', 'Montague') rj.add_edge('Romeo', 'Mercutio') rj.add_edge('Montague', 'Benvolio') rj.add_edge('Montague', 'Escalus') rj.add_edge('Juliet', 'Tybalt') rj.add_edge('Juliet', 'Capulet') rj.add_edge('Juliet', 'Friar Laurence') rj.add_edge('Juliet', 'Romeo') rj.add_edge('Tybalt', 'Capulet') rj.add_edge('Escalus', 'Mercutio') rj.add_edge('Escalus', 'Paris') rj.add_edge('Paris', 'Mercutio') rj.add_edge('Nurse', 'Juliet') rj.add_edge('Capulet', 'Escalus') rj.add_edge('Capulet', 'Paris') return rj def draw_rj(graph): &quot;&quot;&quot;Draw the rj graph to the screen and to a file. &quot;&quot;&quot; nx.draw_networkx(graph) plt.savefig(&quot;romeo-and-juliet.pdf&quot;) plt.show() ### # Problem 2 ### def friends(graph, user): &quot;&quot;&quot;Returns a set of the friends of the given user, in the given graph. &quot;&quot;&quot; # This function has already been implemented for you. # You do not need to add any more code to this (short!) function. return set(graph.neighbors(user)) def friends_of_friends(graph, user): &quot;&quot;&quot;Find and return the friends of friends of the given user. Arguments: graph: the graph object that contains the user and others user: a string Returns: a set containing the names of all of the friends of friends of the user. The set should not contain the user itself or their immediate friends. &quot;&quot;&quot; # creating a new set set_frined = set() # finding frineds for a given user for friend in graph.neighbors(user): # reviewing each specific person for people in graph.neighbors(friend): if people != user: if people not in graph.neighbors(user): if people not in set_frined: set_frined.add(people) return set_frined def common_friends(graph, user1, user2): &quot;&quot;&quot;Finds and returns the set of friends that user1 and user2 have in common. Arguments: graph: the graph object that contains the users user1: a string representing one user user2: a string representing another user Returns: a set containing the friends user1 and user2 have in common &quot;&quot;&quot; # return a set that contains the coomon user return friends(graph, user1) &amp; friends(graph, user2) def number_of_common_friends_map(graph, user): &quot;&quot;&quot;Returns a map (a dictionary), mapping a person to the number of friends that person has in common with the given user. The map keys are the people who have at least one friend in common with the given user, and are neither the given user nor one of the given user's friends. Example: a graph called my_graph and user &quot;X&quot; Here is what is relevant about my_graph: - &quot;X&quot; and &quot;Y&quot; have two friends in common - &quot;X&quot; and &quot;Z&quot; have one friend in common - &quot;X&quot; and &quot;W&quot; have one friend in common - &quot;X&quot; and &quot;V&quot; have no friends in common - &quot;X&quot; is friends with &quot;W&quot; (but not with &quot;Y&quot; or &quot;Z&quot;) Here is what should be returned: number_of_common_friends_map(my_graph, &quot;X&quot;) =&gt; { 'Y':2, 'Z':1 } Arguments: graph: the graph object that contains the user and others user: a string Returns: a dictionary mapping each person to the number of (non-zero) friends they have in common with the user &quot;&quot;&quot; # creating a new dictionary common_map = {} for friend in friends_of_friends(graph, user): # len can not be negative if len(common_friends(graph, user, friend)) &gt; 0: common_map[friend] = len(common_friends(graph, user, friend)) return common_map def number_map_to_sorted_list(map_with_number_vals): &quot;&quot;&quot;Given a dictionary, return a list of the keys in the dictionary. The keys are sorted by the number value they map to, from greatest number down to smallest number. When two keys map to the same number value, the keys are sorted by their natural sort order for whatever type the key is, from least to greatest. Arguments: map_with_number_vals: a dictionary whose values are numbers Returns: a list of keys, sorted by the values in map_with_number_vals &quot;&quot;&quot; # sort both of the key through itemgetter function dict_item = map_with_number_vals.items() sort = sorted(sorted(dict_item, key=itemgetter(0)), key=itemgetter(1), reverse=True) # creating a new list sorted_new_list = [] for item in sort: sorted_new_list.append(item[0]) return sorted_new_list def recommend_by_number_of_common_friends(graph, user): &quot;&quot;&quot; Returns a list of friend recommendations for the user, sorted by number of friends in common. Arguments: graph: the graph object that contains the user and others user: a string Returns: A list of friend recommendations for the given user. The friend recommendation list consists of names/IDs of people in the graph who are not yet a friend of the given user. The order of the list is determined by the number of common friends (people with the most common friends are listed first). In the case of a tie in number of common friends, the names/IDs are sorted by their natural sort order, from least to greatest. &quot;&quot;&quot; common_list = number_of_common_friends_map(graph, user) sorted_friend_list = number_map_to_sorted_list(common_list) return sorted_friend_list ### # Problem 3 ### def influence_map(graph, user): &quot;&quot;&quot;Returns a map (a dictionary) mapping from each person to their influence score, with respect to the given user. The map only contains people who have at least one friend in common with the given user and are neither the user nor one of the users's friends. See the assignment writeup for the definition of influence scores. &quot;&quot;&quot; # creating a new dictionary new_map = {} for name in friends_of_friends(graph, user): score = 0 for person in common_friends(graph, user, name): # calculating the score based on the definition score = score + 1/len(friends(graph, person)) new_map[name] = score return new_map def recommend_by_influence(graph, user): &quot;&quot;&quot;Return a list of friend recommendations for the given user. The friend recommendation list consists of names/IDs of people in the graph who are not yet a friend of the given user. The order of the list is determined by the influence score (people with the biggest influence score are listed first). In the case of a tie in influence score, the names/IDs are sorted by their natural sort order, from least to greatest. &quot;&quot;&quot; new_list = influence_map(graph, user) return number_map_to_sorted_list(new_list) ### # Problem 5 ### def get_facebook_graph(): &quot;&quot;&quot;Builds and returns the facebook graph &quot;&quot;&quot; # creating a graph named facebook facebook = nx.Graph() # reading the file with open('facebook-links.txt') as file: for line in file: line = line.split(' ') facebook.add_edge(int(line[0]), int(line[1])) return facebook def main(): practice_graph = get_practice_graph() # Comment out this line after you have visually verified your practice # graph. # Otherwise, the picture will pop up every time that you run your program. draw_practice_graph(practice_graph) rj = get_romeo_and_juliet_graph() # Comment out this line after you have visually verified your rj graph and # created your PDF file. # Otherwise, the picture will pop up every time that you run your program. draw_rj(rj) ### # Problem 4 ### print(&quot;Problem 4:&quot;) print() # unchanged recoomendation list list_unchange = [] # changed recoomendation list list_change = [] # comparing and finding the difference for people in rj.nodes(): influence_algorithim = recommend_by_influence(rj, people) common_algorithim = recommend_by_number_of_common_friends(rj, people) if influence_algorithim == common_algorithim: list_unchange.append(people) else: list_change.append(people) print(&quot;Unchanged Recommendation:&quot;, sorted(list_unchange)) print(&quot;Changed Recommendation:&quot;, sorted(list_change)) ### # Problem 5 ### facebook = get_facebook_graph() # assert len(facebook.nodes()) == 63731 # assert len(facebook.edges()) == 817090 ### # Problem 6 ### print() print(&quot;Problem 6:&quot;) print() # Get user id that is mutiple 1000 list. user_mutiple_1000 = [] # finding all users that is the mutiple of 1000 in the dataset for people in list(facebook.nodes()): if people % 1000 == 0: user_mutiple_1000.append(people) # sort in ascending order of userID user_mutiple_1000.sort() # using number of frined function for friend in user_mutiple_1000: friend_list = recommend_by_number_of_common_friends(facebook, friend) # get the first 10 result by using list slicing print(friend, '(by number_of_common_friends):', friend_list[:10]) ### # Problem 7 ### print() print(&quot;Problem 7:&quot;) print() # using influence score function in the user listed that we created above for people in user_mutiple_1000: influence_recommend = recommend_by_influence(facebook, people) # get the first 10 result by using list slicing print(people, '(by influence):', influence_recommend[:10]) ### # Problem 8 ### print() print(&quot;Problem 8:&quot;) print() same_number = 0 diff_number = 0 for people in user_mutiple_1000: influence_method = recommend_by_influence(facebook, people) common_method = recommend_by_number_of_common_friends(facebook, people) if influence_method == common_method: same_number += 1 else: diff_number += 1 print('Same:', same_number) print('Different:', diff_number) if __name__ == &quot;__main__&quot;: main() ","link":"https://irisdin.github.io/post/ZT9SQzev5/"},{"title":"# çš®å›Šï¼Œé­‚çµï¼›æ˜¯çŸ³å¤´è¿˜æ˜¯å‰ªåˆ€","content":"çŒœæ‹³æ°¸è¿œæ˜¯ä¸å¯æ§çš„æœªçŸ¥æ•°ï¼ŒçŸ³å¤´æ°¸è¿œæ˜¯å‰ªåˆ€çš„èµ¢å®¶ï¼Œçš®å›Šå’Œé­‚çµä¸­ï¼Œå“ªä¸€ä¸ªå¯ä»¥ç”Ÿå­˜ å¤§ä¼—å®¡ç¾ åœ¨å½“ä»Šæ—¶ä»£ï¼Œå§£å¥½çš„å®¹è²Œæˆäº†æŸç§è¯è¯­æƒï¼Œâ½½â¼¤ä¼—çš„å®¡ç¾æ ‡å‡†ä¹Ÿä¼¼ä¹è¢«ç¦é”¢äº†å¹¶å‘ˆâ¼€ç›´ä¸‹é™çš„è¶‹åŠ¿ã€‚ ä»¥æŸçŸ­è§†é¢‘å¹³å°ä¸ºä¾‹ï¼Œâ½¤â¼¾æ€»æ˜¯ä¹æ­¤ä¸ç–²çš„â½¤â¼€æ ·çš„æ¨¡æ¿ï¼Œæ²‰è¿·äºæµâ½”çº¿å¼çš„å®¡ç¾ã€‚åœ¨èµ„æœ¬çš„æ¨æ³¢åŠ© æ¾œä¸‹ï¼Œâ€œâ½©ï¼Œå¹¼ï¼Œç˜¦â€ çš„ä¸»æµå®¡ç¾é€æ¸å½¢æˆï¼Œå®¡ç¾å¤šå…ƒåŒ–ä¹Ÿåœ¨ç¤¾ä¼šä¸­æ…¢æ…¢è¢«åå™¬ã€‚åœ¨å±…ä¼Š.å¾·æ³¢çš„ã€Šæ™¯è§‚ç¤¾ä¼šã€‹ä¸­æåˆ°ï¼šâ€œåœ¨â½£æ´»ä¸­ï¼Œå¦‚æœâ¼ˆä»¬å®Œå…¨é¡ºä»äºæ™¯è§‚çš„ç»Ÿæ²»ï¼Œé€æ­¥è¿œç¦»â¼€åˆ‡å¯èƒ½çš„åˆ‡â¾ä½“éªŒï¼Œå¹¶ ç”±æ­¤è¶Šæ¥è¶Šéš¾ä»¥æ‰¾åˆ°ä¸ªâ¼ˆçš„å–œå¥½ï¼Œé‚£ä¹ˆï¼Œè¿™ç§çŠ¶æ€å°±â½†å¯é¿å…åœ°å°±ä¼šé€ æˆå¯¹ä¸ªæ€§çš„æŠ¹æ€â€ã€‚è¿™å¥è¯â½† ç–‘æ˜¯å¯¹å½“ä»Šâ¼¤ä¼—å®¡ç¾åŒåŒ–çš„å°è¯ã€‚æ‰€è°“çš„â¼¤ä¼—å®¡ç¾å’Œè¢«çŒè¾“çš„ç•¸å½¢çš„å®¡ç¾ï¼Œç©¶ç«Ÿå›°ä½äº†å¤šå°‘â¼ˆï¼Œè®©å¤š å°‘â¼ˆé™·â¼Šäº†æ‰€è°“çš„å®¹è²Œç„¦è™‘å½“ä¸­ï¼Ÿ æˆ‘ä»¬éœ€è¦â€œå®¡ç¾å¤šå…ƒåŒ–â€/æµ…è°ˆä¸åŒâ¼ˆå¯¹äºç¾çš„â»…è§£ ç¾æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆï¼Œç¾ä¸åº”è¯¥è¢«å®šä¹‰ï¼Œâ½©å¹¼ç˜¦ä¹Ÿä¸åº”è¯¥æˆä¸ºç¾ä¸½çš„æ ‡ç­¾ ç¾ä¸ä»…ä»…ä»…é™äºâ€â½©ï¼Œå¹¼ï¼Œç˜¦â€œï¼Œå®ƒåº”è¯¥æ˜¯å¤šå…ƒåŒ–çš„ã€‚è¥¿â½…å¯¹äºå®¡ç¾ä¼¼ä¹æ›´åŠ å¤šå…ƒåŒ–ï¼Œä¸¹å‡¤çœ¼æ‰å¹³è„¸ å¯ä»¥æ˜¯ç¾çš„ï¼Œâ¼©â»¨â¾Šå¥åº·è‚¤â¾Šä¹Ÿå¯ä»¥æ˜¯ç¾çš„ï¼Œæ²¡æœ‰è‚Œâ¾çš„ç”·â½£ä¹Ÿæ˜¯å¯Œæœ‰é­…â¼’çš„ã€‚ç¾ä¸½æ˜¯ä¸»è§‚çš„ï¼Œæ¯ä¸ª â¼ˆéƒ½æœ‰ç€ä¸â¼€æ ·çš„ç­”æ¡ˆã€‚å‚ä¸åœ†æ¡Œçš„å„ä½æ¢è®¨äº†å„â¾ƒè®¤ä¸ºçš„ç¾ï¼Œæœ‰â¼ˆè¯´ï¼šâ€œç›¸å¤„èµ·æ¥å’Œè°ï¼Œèˆ’æœçš„â¼ˆ å°±æ˜¯ç¾ä¸½çš„ï¼Œæˆ‘å¹¶ä¸è®¤åŒå½“ä»Šæ‰€æ¨å´‡çš„æµâ½”çº¿å¼çš„ç¾ä¸½â€ã€‚ä¹Ÿæœ‰â¼ˆè¯´äº†ä»–æ‰€è®¤ä¸ºçš„ç¾çš„ç‰¹è´¨ï¼šè¾ƒâ¾¼çš„ â¾â¾¼ï¼ŒæŒºæ‹”çš„â¿â¼¦...åŒæ ·çš„åœ¨å¨±ä¹åœˆâ¾¥ï¼Œæœ‰äº›â¼ˆå–œæ¬¢ä»¥ç¡¬æ±‰å½¢å¼å‡ºç°çš„æœ‰æ£±â»†çš„ç”·æ€§ç¾ï¼Œä¹Ÿæœ‰â¼ˆå–œæ¬¢ æŸ”ç¾ç²¾è‡´çš„ç¾ä¸½ã€‚èŠ­â½ï¼Œä½œä¸ºä¸–ç•Œä¸Šå¯¹äºå­©â¼¦æœ€å…·æœ‰å½±å“â¼’çš„ç©å¶ï¼Œä¼ é€’äº†â¼€ä¸ªå¥åº·ï¼Œå¤šå…ƒåŒ–çš„å®¡ ç¾ã€‚èŠ­â½æ²¡æœ‰è¢«å¡‘é€ æˆå®Œç¾çš„å½¢è±¡ï¼Œâ½½æ©±çª—â¾¥æ‘†æ”¾ç€æ˜¯ä¸åŒè‚¤â¾Šï¼Œâ¾â¾¼ï¼Œâ¾æçš„å„å¼å„æ ·çš„å¨ƒå¨ƒã€‚ æ‰€ä»¥ï¼Œæ´»åœ¨å®¡ç¾â¾¥æ˜¯æ¼‚äº®ï¼Œæ´»å‡ºâ¾ƒâ¼°æ‰æ˜¯çœŸæ­£çš„ç¾ã€‚ ç°åœ¨æ˜¯â¼€ä¸ªçœ‹è„¸çš„æ—¶ä»£å— è¢«å¤–è¡¨æ‰€å¸å¼•æ˜¯â½£ç‰©æœ¬èƒ½ã€‚åœ¨â¾ƒç„¶ç•Œï¼ŒåŠ¨ç‰©çš„å¤–è¡¨â¾¼â¼¤å¯¹ç§°ã€å¼ºå£®å¥åº·ï¼Œé€šå¸¸æ„å‘³ç€æ‹¥æœ‰è‰¯å¥½åŸºå› ï¼Œ ä¼šè·å¾—æ›´å¤šçš„ç¹è¡æœºä¼šã€‚åˆ°äº†â¼ˆç±»ç¤¾ä¼šâ¾¥ï¼Œçœ‹è°çš„è„¸ï¼Œçœ‹ä»€ä¹ˆæ ·çš„è„¸ï¼Œåˆ™æ˜¯æ€§åˆ«æƒåˆ©çš„è¡¨ç°ã€‚ é¢œå€¼å³æ­£ä¹‰è¿™æ ·çš„å¥â¼¦ä¹Ÿæœ‰åœ¨ç¤¾ä¼šä¸­çœŸå®çš„ä¾‹â¼¦ã€‚æœ‰â¼€ä¸ªå«åšå¸‚æ¡¥è¾¾ä¹Ÿçš„â½‡æœ¬ç½ªçŠ¯åœ¨07å¹´çš„æ—¶å€™â½¤æ å…¶æ®‹å¿çš„â¼¿æ³•å¥¸æ€äº†â½†è¾œçš„è‹±å›½â¼¥å­©æ—èµ›Â·å®‰å¦®Â·éœå…‹å¹¶æŠ˜ç£¨äº†å®‰å¦®â¼—â¼ä¸ªâ¼©æ—¶ã€‚åœ¨ç°å®ä¸­ï¼Œè¿™æ ·çš„æ€ â¼ˆçŠ¯å°±åº”è¯¥é­å—ç¤¾ä¼šçš„å”¾éª‚ï¼Œå¤„ä»¥æœ€ä¸¥å‰çš„é…·åˆ‘ã€‚ä½†æ˜¯ä»–çš„è„¸æ”¹å˜äº†â¼¤å®¶å¯¹ä»–çš„çœ‹æ³•ï¼Œåœ¨ä»–çš„ç…§â½šè¢« åª’ä½“å…¬å¸ƒå‡ºæ¥åï¼Œå¾ˆå¤šâ¼¥æ€§è¿·ä¸Šäº†ä»–æ¼«ç”»â¼€èˆ¬çš„è„¸åºã€‚æ›´ä¸ºè’å”çš„æ˜¯ï¼Œè¿™äº›â¼¥æ€§ä¸ºè¿™åâ¼—æ¶ä¸èµ¦çš„æ€ â¼ˆçŠ¯æˆâ½´äº†åæ´ä¼šï¼Œå¹¶è¦æ±‚åˆ¤ä»–â½†ç½ªã€‚åœ¨æ–°é—»å­¦ä¸Šæœ‰ä¸ªè¯ï¼Œå«â€œèˆ†è®ºå¤±ç„¦â€ï¼Œé€šä¿—çš„è¯´æ˜¯è¯´äº‹ä»¶ç»è¿‡ â½¹ç»œå‘å±•ï¼Œå…¬ä¼—è®¨è®ºåä½¿èˆ†è®ºè¢«â¼€â½…ä¸»å¯¼ï¼Œå¯¼è‡´åŸæœ¬çš„ä¸»é¢˜å‘å…¶ä»–çš„â½…å‘å‘å±•ã€‚â¼ˆä»¬æŠŠå…³æ³¨ç‚¹ä»å‘½æ¡ˆ è½¬ç§»åˆ°äº†â€è„¸â€œä¸Šï¼Œè¿™åæ˜ äº†çœ‹è„¸çš„ä¸–ç•Œï¼Œä½†ä¹ŸåŒæ ·æ˜¯å¯æ‚²çš„ã€‚ â½ªå›Šç»™â¼ˆå¸¦æ¥è§†è§‰å®¡ç¾ï¼Œè‹¥è¿™å‰¯â½ªå›Šæ²¾æ»¡â¾è…¥ï¼Œä»–è¿˜æ˜¯ç¾çš„å—ï¼Ÿ æ‰€ä»¥æˆ‘ä»¬æ˜¯ä¸æ˜¯åªè¦â¼€å‘³çš„è¿½æ±‚ç¾ä¸½çš„â½ªå›Šå°±èƒ½æ”¹å¤©æ¢å‘½å‘¢ï¼Ÿå¨±ä¹æ–°é—»â¾¥ï¼Œä¾é é¢œå€¼å¸å¼•å¯Œâ¼ˆï¼Œé€šè¿‡å©šå§»æ”¹å˜å‘½è¿çš„æ•…äº‹ï¼Œæˆ‘ä»¬å½“ç„¶çœ‹è¿‡å¾ˆå¤šï¼Œä½†çœŸå®æƒ…å†µè¦â½å•çº¯çœ‹è„¸å¤æ‚ï¼šæˆ‘ä»¬ä¸”ä¸è¯´å†ç¾ä¸½çš„å¤–è¡¨ ä¹Ÿä¼šæœ‰å®¡ç¾ç–²åŠ³çš„â¼€å¤©ã€‚ä¼šç©â¾¦é’±æ¸¸æˆçš„å¯Œâ¼ˆï¼Œå½“ç„¶éƒ½æ˜â½©è¿™ä¸ªé“ç†ã€‚æ‰€ä»¥ï¼Œåœ¨èŠ±è¾¹æ–°é—»â¾¥ï¼Œä»–ä»¬ä¼šé¢‘ç¹åœ°æ›´æ¢æ‰€è°“â¼¥æœ‹å‹ï¼Œä½†èƒ½è®©ä»–ä»¬è®¤ä¸ºå€¼å¾—ä»¥å©šå§»å½¢å¼æ¥ä¹°å•çš„å¯¹è±¡ï¼Œå°±ç»ä¸ä»…ä»…æ˜¯çœ‹è„¸è¿™ä¹ˆç®€ å•ã€‚â¾¯å¯¹â¼€ä¸ªä¼šä¸æ–­è´¬å€¼â½½ä¸”ä¼šå¼•èµ·å®¡ç¾ç–²åŠ³çš„ä¸œè¥¿ï¼Œç¨å¾®å…·å¤‡è´¢åŠ¡çŸ¥è¯†çš„â¼ˆå°±æ‡‚å¾—ï¼Œæœ€å¥½çš„ç­–ç•¥ æ˜¯â€œåªç§Ÿâ½½ä¸ä¹°â€ã€‚åœ¨â¼¯ä½œä¸­ä¹Ÿæ˜¯â¼€æ ·ï¼Œå…¬å¸çš„â»“æœŸå‘å±•ç¦»ä¸å¼€æœ‰èƒ½â¼’çš„å‘˜â¼¯å¸¦é¢†å›¢é˜Ÿã€‚ä¸šç»©å’Œèƒ½â¼’ å¯ä»¥å†³å®šâ¼€ä¸ªå…¬å¸çš„åˆ©æ¶¦å’Œâ»“æœŸå‘å±•ï¼Œä½†æ˜¯é å•çº¯çš„ç¾è²Œæ¥è¿è¥å•†ä¸šçš„ç†è®ºæ˜¯ä¸å­˜åœ¨çš„ï¼Œä¹Ÿæ˜¯ä¸åˆç† çš„ã€‚æˆ‘ä»¬å¾—å‡ºçš„ç»“è®ºæ˜¯ï¼Œè¿™æ˜¯ä¸ªâ¼€å¼€å§‹çœ‹è„¸ï¼Œä½†å¤„ä¹…çœ‹è„‘çš„ä¸–ç•Œã€‚â½†å¯åšâ¾®å¥½çœ‹çš„â½ªå›Šå°±åƒå……å€¼äº†ä¼šå‘˜â¼€ æ ·ï¼Œæ‹¥æœ‰æ›´å¤šçš„æƒâ¼’å’Œä¼˜åŠ¿ã€‚ä½†æ˜¯è¿™ä¸æ˜¯â¼€ä¸ªâ€åªâ€œçœ‹è„¸çš„æ—¶ä»£ã€‚â€œçœ‹è„¸â€åªæ˜¯â¼€ä¸ªå¼€å§‹ã€‚è¦ç»´æŒå·² ç»å–å¾—çš„ä¼˜åŠ¿ï¼Œå¿…ç„¶å¾—æœ‰â¼€äº›å…¶ä»–çš„ä»·å€¼å’Œèµ„æºæ¥ä¸°å¯Œâ¾ƒâ¼°ï¼Œâ¾„å°‘æ˜¯å¯¹å†²é¢œå€¼çš„ä¸‹æ»‘ã€‚ä»å¦â¼€ä¸ªâ½…å‘ çœ‹ï¼Œä¸ºä»€ä¹ˆæœ‰â¼€äº›åˆçœ‹è²Œä¸æƒŠè‰³çš„â¼ˆï¼Œä¼šè¶Šæ¥è¶Šæœ‰é­…â¼’ï¼Œç”šâ¾„ä½ å¼€å§‹è§‰å¾—ä»–â»“å¾—ä¹Ÿå¾ˆç¾äº†ï¼Œä¹Ÿæ˜¯è¿™ä¸ª åŸå› ã€‚æœ‰â€œæœ‰è¶£çµé­‚â€è¿›é©»ä¹‹åï¼Œå¥½çœ‹çš„â½ªå›Šå°±ä¸ä¼šåƒç¯‡â¼€å¾‹ã€‚çµé­‚ä¹å‘³ï¼Œåªä¼šæ˜¯â€œç”»â½ªâ€ã€‚ credit to editor: yunyi ding /zizhan li/ chen zhang/ xinzhe he team levervision ","link":"https://irisdin.github.io/post/54PDWzCQ0/"},{"title":"Fraud detection pratice","content":"readme linkğŸ”» uw cse only for practice purpose/plagiarism is not allowedğŸ”» cse misconduct guideline # importing the packages we used for this assignment import utils # noqa: F401, do not remove if using a Mac import matplotlib.pyplot as plt import random import csv def extract_election_vote_counts(filename, column_names): &quot;&quot;&quot; Returns a list of integers that contains the values in those columns from every row (the order of the integers does not matter). examples: extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Ahmadinejad&quot;, &quot;Rezai&quot;, &quot;Karrubi&quot;, &quot;Mousavi&quot;]) Should return: [1131111, 16920, 7246, 837858, 623946, 12199, 21609, 656508, ... Arguments: file_name: the election csv file column_name: list of column names for each province Returns: a list of integers that contains the values in those columns *update for problem 8: When data is missing, our calculation should ignore that data. Do not transform it into a zero. &quot;&quot;&quot; # creating an new empty list cleaned_data = [] # open the file election_file = open(filename) # reading lines from a csv file for line in csv.DictReader(election_file): for characters in column_names: # ignore the empty space in the csv file if len(line[characters]) &gt; 0: # replace unwanted characters with the empty string number = int(line[characters].replace(&quot;,&quot;, &quot;&quot;)) cleaned_data.append(int(number)) election_file.close() # Returns a list of integers that contains the values we want return cleaned_data def ones_and_tens_digit_histogram(numbers): &quot;&quot;&quot; taking as inputs a list of numbers and produce as ouput a list of 10 numbers. examples: the call ones_and_tens_digit_histogram([127, 426, 28, 9, 90]) should return [0.2, 0.0, 0.3, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.2] Arguments: numbers: a list of numbers Returns: In the returned list, the value at index i is the frequency with which digit i appeared in the ones place OR the tens place in the input list. In the input, in a number that is less than 10, such as 3, the tens place is implicitly zero. That is, 3 must be treated as 03. &quot;&quot;&quot; # creating new empty list ten_list = [] convert_num_list = [] # converting the number into two digits for n in numbers: n = n % 100 convert_num_list.append(n) for i in range(0, 10): frequency = 0 # finding the tens place and ones place for num in convert_num_list: if int(num // 10) == i: frequency += 1 if int(num % 10) == i: frequency += 1 # finding the total length of digit for calculation digits_length = len(numbers) * 2 # returning the list based by the calculation ten_list.append(frequency/(digits_length)) return ten_list def plot_iran_least_digits_histogram(histogram): &quot;&quot;&quot;&quot;&quot; Takes a histogram (as created by ones_and_tens_digit_histogram) Graphs the frequencies of the ones and tens digits for the Iranian election data. Save your plot to a file named iran-digits.png using plt.savefig. Arguments: histogram: the list of numbers with the frequencies of the ones and tens digits Return: The function should not return anything but we need to show and save the plot &quot;&quot;&quot; # creating the plot based on the example x_axis = list(range(0, 10)) plt.title('Distribution of last two digits in Iranian dataset') plt.xlabel(&quot;Digit&quot;) plt.ylabel(&quot;Frequency&quot;) # The ideal line represents the uniform distribution ideal_line = [0.1] * 10 # creating the graph with the detail that given plt.plot(x_axis, ideal_line, color='blue', label='ideal') plt.plot(x_axis, histogram, color='orange', label='iran') plt.legend(loc='upper left') plt.savefig(&quot;iran-digits.png&quot;) plt.show() plt.clf() return None def plot_distribution_by_sample_size(): &quot;&quot;&quot; Creates five different collections (one of size 10, another of size 50, then 100, 1000, and 10,000) of random numbers where every element in the collection is a different random number x such that 0 &lt;= x &lt; 100. Argument: / Return: The function should not return anything but we need to show and save the graph. &quot;&quot;&quot; # graph the ideal line(* based on the format given) ideal_line = [0.1] * 10 # creating differnt collections of random numbers plt.plot(ideal_line, color='blue', label='ideal') for i in [10, 50, 100, 1000, 10000]: # the random_list function is a helper function that generate # random data graph = ones_and_tens_digit_histogram(random_list(i)) plt.plot(graph, label=str(i) + 'random numbers') # creating other details based on the example given plt.title('Distribution of last two digits in randomly generated samples') plt.xlabel(&quot;Digit&quot;) plt.ylabel(&quot;Frequency&quot;) plt.legend(loc='upper left') plt.savefig(&quot;random-digits.png&quot;) plt.show() plt.clf() def mean_squared_error(numbers1, numbers2): &quot;&quot;&quot; finding way to computationally determine how similar two lines are. determine whether the difference between lines A and B is larger or smaller than the difference between lines C and D. Argument: two lists of numbers Return: the mean squared error between the lists. &quot;&quot;&quot; mean_squared_error = 0.0 # finding the mse based on the definition for i in range(len(numbers1)): mean_squared_error += (numbers1[i] - numbers2[i])**2 mean_squared_error = mean_squared_error / (float)(len(numbers1)) return mean_squared_error def calculate_mse_with_uniform(histogram): &quot;&quot;&quot; Argument: histogram(as created by ones_and_tens_digit_histogram) Return: Returns the mean squared error of the given histogram with the uniform distribution. E.G.: call: calculate_mse_with_uniform(histogram) Should return: 0.000739583333333 &quot;&quot;&quot; # calling the mean square error function to return the # number we want uniform__his_mse = mean_squared_error(histogram, [0.1] * 10) return uniform__his_mse def random_list(number): &quot;&quot;&quot;&quot; This is a helper function to avoid duplicate code. The function randomly generated number that 0 &lt;= number &lt; 100 * this helper function can be used in the function plot_distribution_by_sample_size and using to compraing the election MSE with the sample Return: a list of random numbers that 0&lt;= number &lt; 100 &quot;&quot;&quot; # creating a new empty list random_lst = [] # generate random number x with the given range for x in range(number): random_lst.append(random.randint(0, 99)) return random_lst def Mse_election_to_samples(mse_election, election_datapoints, specific_election): &quot;&quot;&quot; this is a helper function to avoid dupicate codes when we try to implement different dataset(E.G: different countries election dataset) Determine how many of the 10,000 random MSEs are larger than or equal to MSE for election Determine how many of the 10,000 random MSEs are smaller than the MSE for election Determine the election null hypothesis rejection level Print all of these values Arguments:mse_election : the mean sqaured error of each specific eletion election_datapoints: number of datapoints(*we can not haedcode for that) specific_election: the title of file we used for the print statement return: this function should not return anything &quot;&quot;&quot; small_quan = 0 large_quan = 0 # we need to generate 10000 random MSEs for i in range(0, 10000): # calling the helper function to create random list random = random_list(election_datapoints) # finding the frequency his = ones_and_tens_digit_histogram(random) # finding the mean squared error mse = calculate_mse_with_uniform(his) # comparing of the sample mse with the real election mse if (mse &gt;= mse_election): large_quan += 1 else: small_quan += 1 # calculating the p value based on the definition # using the p value to compare p_value = large_quan / 10000 # the print statment we used for differnt elections # writing based on the format that given print(specific_election, &quot;election MSE:&quot;, mse_election) print(&quot;Quantity of MSEs larger than or equal to the&quot;, specific_election, &quot;election MSE:&quot;, large_quan) print(&quot;Quantity of MSEs smaller than the&quot;, specific_election, &quot;election MSE:&quot;, small_quan) print(specific_election, &quot;election null hypothesis rejection level p:&quot;, p_value) return None def compare_iran_mse_to_samples(iran_mse, number_of_iran_datapoints): &quot;&quot;&quot; Determine how many of the 10,000 random MSEs are larger than or equal to the Iran MSE Determine how many of the 10,000 random MSEs are smaller than the Iran MSE Determine the Iranian election null hypothesis rejection level Print all of these values after the Iranian MSE. Arguments: the Iranian MSE (as computed by calculate_mse_with_uniform) the number of data points in the Iranian dataset Return: This function should not return anything. &quot;&quot;&quot; # calling the helper function above to generate iran result MSE_election_to_samples(iran_mse, number_of_iran_datapoints, &quot;2009 Iranian&quot;) return None def compare_us_mse_to_samples(us_mse, number_of_us_datapoints): &quot;&quot;&quot; works the same as the compare_iran_mse_to_samples function * just replacing the dataset to real us election &quot;&quot;&quot; # calling the helper function above to generate iran result MSE_election_to_samples(us_mse, number_of_us_datapoints, &quot;2008 United States&quot;) return None # The code in this function is executed when this # file is run as a Python program # calling the function that written above def main(): iran_election = ['Ahmadinejad', 'Rezai', 'Karrubi', 'Mousavi'] vote_count_iran = extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, iran_election) iran_his = ones_and_tens_digit_histogram(vote_count_iran) iran_error = calculate_mse_with_uniform(iran_his) plot_iran_least_digits_histogram(iran_his) plot_distribution_by_sample_size() # we do not need to hard code the number of datapoints compare_iran_mse_to_samples(iran_error, len(vote_count_iran)) # formating requirement print() # * using other dataset(the code works the same way as above) us_election = [&quot;Obama&quot;, &quot;McCain&quot;, &quot;Nader&quot;, &quot;Barr&quot;, &quot;Baldwin&quot;, &quot;McKinney&quot;] vote_count_us = extract_election_vote_counts(&quot;election-us-2008.csv&quot;, us_election) us_his = ones_and_tens_digit_histogram(vote_count_us) us_error = calculate_mse_with_uniform(us_his) compare_us_mse_to_samples(us_error, len(vote_count_us)) if __name__ == &quot;__main__&quot;: main() testing import fraud_detection as fd import math def test_extract_election_vote_counts(): actual = fd.extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Ahmadinejad&quot;, &quot;Rezai&quot;, &quot;Karrubi&quot;, &quot;Mousavi&quot;])[:8] expected = [1131111, 16920, 7246, 837858, 623946, 12199, 21609, 656508] assert actual == expected actual = fd.extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Ahmadinejad&quot;])[:10] expected = [1131111, 623946, 325911, 1799255, 199654, 299357, 3819495, 359578, 285984, 2214801] assert actual == expected actual = fd.extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Karrubi&quot;])[:10] expected = [7246, 21609, 2319, 14579, 7471, 3563, 67334, 4127, 928, 13561] assert actual == expected actual = actual = fd.extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Mousavi&quot;])[:10] expected = [837858, 656508, 302825, 746697, 96826, 177268, 3371523, 106099, 90363, 884570] assert actual == expected print(&quot;test_extract_election_vote_counts passed.&quot;) def test_ones_and_tens_digit_histogram(): actual = fd.ones_and_tens_digit_histogram([127, 426, 28, 9, 90]) expected = [0.2, 0.0, 0.3, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.2] actual = fd.ones_and_tens_digit_histogram([0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765]) expected = [0.21428571428571427, 0.14285714285714285, 0.047619047619047616, 0.11904761904761904, 0.09523809523809523, 0.09523809523809523, 0.023809523809523808, 0.09523809523809523, 0.11904761904761904, 0.047619047619047616] actual = fd.ones_and_tens_digit_histogram([3, 5, 15, 16, 18]) expected = [0.2, 0.3, 0.0, 0.2, 0.0, 0.1, 0.1, 0.0, 0.1, 0.0] actual = fd.ones_and_tens_digit_histogram([123, 88, 1314, 279, 236556]) expected = [0.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.1] actual = fd.ones_and_tens_digit_histogram([10, 10]) expected = [0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] for i in range(len(actual)): assert math.isclose(actual[i], expected[i]) print(&quot;test_ones_and_tens_digit_histogram passed.&quot;) def test_mean_squared_error(): assert fd.mean_squared_error([1, 4, 9], [6, 5, 4]) == 17.0 assert fd.mean_squared_error([23, 37], [33, 49]) == 122.0 assert fd.mean_squared_error([123, 456, 789], [133, 466, 799]) == 100.0 print(&quot;test_mean_squared_error passed.&quot;) def test_calculate_mse_with_uniform(): dataset = fd.extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Ahmadinejad&quot;, &quot;Rezai&quot;, &quot;Karrubi&quot;, &quot;Mousavi&quot;]) frequency = fd.ones_and_tens_digit_histogram(dataset) actual = fd.calculate_mse_with_uniform(frequency) expected = 0.000739583333333 assert math.isclose(actual, expected) print(&quot;test_calculate_mse_with_uniform passed.&quot;) def main(): test_extract_election_vote_counts() test_ones_and_tens_digit_histogram() test_mean_squared_error() test_calculate_mse_with_uniform() print(&quot;All test passed.&quot;) if __name__ == &quot;__main__&quot;: main() ","link":"https://irisdin.github.io/post/HsYyfFoIA/"},{"title":"K-Means algorithms practice","content":"readme linkğŸ”» uw cse only for practice purpose/plagiarism is not allowedğŸ”» [cse misconduct guideline](https://www.cs.washington.edu/academics/misconduct import matplotlib.pyplot as plt # noqa: E402 import os # noqa: E402 import math # noqa: E402 from utils import converged, plot_2d, plot_centroids, assert_equals, \\ read_data, load_centroids, write_centroids_tofile # noqa: E402 # problem for students def euclidean_distance(point1, point2): &quot;&quot;&quot;Calculate the Euclidean distance between two data points. Arguments: point1: a non-empty list of floats representing a data point point2: a non-empty list of floats representing a data point Returns: the Euclidean distance between two data points Example: Code: point1 = [1.1, 1, 1, 0.5] point2 = [4, 3.14, 2, 1] print(euclidean_distance(point1, point2)) Output: 3.7735394525564456 &quot;&quot;&quot; point_distance = 0.0 for i in range(len(point1)): # using the formula to find the distance between two points point_distance += (point1[i] - point2[i]) ** 2 return math.sqrt(point_distance) # problem for students def get_closest_centroid(point, centroids_dict): &quot;&quot;&quot;Given a datapoint, finds the closest centroid. You should use the euclidean_distance function (that you previously implemented). Arguments: point: a list of floats representing a data point centroids_dict: a dictionary representing the centroids where the keys are strings (centroid names) and the values are lists of centroid locations Returns: a string as the key name of the closest centroid to the data point Example: Code: point = [0, 0, 0, 0] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [2, 2, 2, 2]} print(get_closest_centroid(point, centroids_dict)) Output: centroid1 &quot;&quot;&quot; max = 100000000000000000000000000000 close = '' for i in centroids_dict.keys(): d = euclidean_distance(point, centroids_dict[i]) # Comparing until we find the cloest distance if d &lt; max: max = d close = i return close # problem for students def update_assignment(list_of_points, centroids_dict): &quot;&quot;&quot;Assign all data points to the closest centroids. You should use the get_closest_centroid function (that you previously implemented). Arguments: list_of_points: a list of lists representing all data points centroids_dict: a dictionary representing the centroids where the keys are strings (centroid names) and the values are lists of centroid locations Returns: a new dictionary whose keys are the centroids' key names and values are lists of points that belong to the centroid. If a given centroid does not have any data points closest to it, do not include the centroid in the returned dictionary Example: Code: list_of_points = [[1.1, 1, 1, 0.5], [4, 3.14, 2, 1], [0, 0, 0, 0]] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [2, 2, 2, 2]} print(update_assignment(list_of_points, centroids_dict)) Output: {'centroid1': [[1.1, 1, 1, 0.5], [0, 0, 0, 0]], 'centroid2': [[4, 3.14, 2, 1]]} &quot;&quot;&quot; # creating a new dictionary dic = {} ls = list(centroids_dict.keys()) for i in ls: dic[i] = [] for j in list_of_points: s = get_closest_centroid(j, centroids_dict) dic[s].append(j) dic1 = {} for i in dic.keys(): if len(dic[i]) != 0: dic1[i] = dic[i] return dic1 # problem for students def mean_of_points(list_of_points): &quot;&quot;&quot;Calculate the mean of a given group of data points. You should NOT hard-code the dimensionality of the data points). Arguments: list_of_points: a list of lists representing a group of data points Returns: a list of floats as the mean of the given data points Example: Code: list_of_points = [[1.1, 1, 1, 0.5], [4, 3.14, 2, 1], [0, 0, 0, 0]] print(mean_of_points(list_of_points)) Output: [1.7, 1.3800000000000001, 1.0, 0.5] &quot;&quot;&quot; mean = [] for x in range(len(list_of_points[0])): sum = 0 for y in range(len(list_of_points)): sum += list_of_points[y][x] answer = sum / len(list_of_points) mean.append(answer) return(mean) # problem for students def update_centroids(assignment_dict): &quot;&quot;&quot;Update centroid locations as the mean of all data points that belong to the cluster. You should use the mean_of_points function (that you previously implemented). Arguments: assignment_dict: a dictionary whose keys are the centroids' key names and values are lists of points that belong to the centroid. It is the dictionary returned by update_assignment function. Returns: A new dictionary representing the updated centroids. If a given centroid does not have any data points closest to it, do not include the centroid in the returned dictionary. Example: Code: assignment_dict = {'centroid1': [[1.1, 1, 1, 0.5], [0, 0, 0, 0]], 'centroid2': [[4, 3.14, 2, 1]]} print(update_centroids(assignment_dict)) Output: {'centroid1': [0.55, 0.5, 0.5, 0.25], 'centroid2': [4.0, 3.14, 2.0, 1.0]} &quot;&quot;&quot; new_version = {} for a, b in assignment_dict.items(): new_version[a] = mean_of_points(b) return new_version # ---------------------------------------------------------- # HELPER FUNCTIONS def setup_data_centroids(): &quot;&quot;&quot;Creates are returns data for testing k-means methods. Returns: list_of_points, a list of data points centroids_dict1, two 4D centroids centroids_dict2, two 4D centroids &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### list_of_points = [ [-1.01714716, 0.95954521, 1.20493919, 0.34804443], [-1.36639346, -0.38664658, -1.02232584, -1.05902604], [1.13659605, -2.47109085, -0.83996912, -0.24579457], [-1.48090019, -1.47491857, -0.6221167, 1.79055006], [-0.31237952, 0.73762417, 0.39042814, -1.1308523], [-0.83095884, -1.73002213, -0.01361636, -0.32652741], [-0.78645408, 1.98342914, 0.31944446, -0.41656898], [-1.06190687, 0.34481172, -0.70359847, -0.27828666], [-2.01157677, 2.93965872, 0.32334723, -0.1659333], [-0.56669023, -0.06943413, 1.46053764, 0.01723844] ] centroids_dict1 = { &quot;centroid1&quot;: [0.1839742, -0.45809263, -1.91311585, -1.48341843], &quot;centroid2&quot;: [-0.71767545, 1.2309971, -1.00348728, -0.38204247], } centroids_dict2 = { &quot;centroid1&quot;: [0.1839742, -0.45809263, -1.91311585, -1.48341843], &quot;centroid2&quot;: [10, 10, 10, 10], } return list_of_points, centroids_dict1, centroids_dict2 # ---------------------------------------------------------- # TESTS def test_euclidean_distance(): &quot;&quot;&quot;Function for verifying if euclidean_distance is correctly implemented. Will throw an error if it isn't. &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### # simple case point1 = [0, 0, 0, 0] point2 = [1, 1, 1, 1] assert_equals(2, euclidean_distance(point1, point2)) # negative point1 = [-1, -1, -1, -1] point2 = [-5, -3, -1, -1] assert_equals(math.sqrt(20), euclidean_distance(point1, point2)) # floats point1 = [1.1, 1, 1, 0.5] point2 = [4, 3.14, 2, 1] assert_equals(math.sqrt(14.2396), euclidean_distance(point1, point2)) # long version point1 = [-0.451, 0.535, 1.031, 1.097, 0.59, -0.435, 1.934, 0.227, 1.026, 0.427, 0.267, -1.482, -0.636, 0.354, -0.675, -0.751, -0.719, -0.454, -1.262, -0.326, -0.608, -0.22, 0.354, 1.048, -0.92, -0.027, 0.328, 1.397, 0.05, -0.125, 0.329, 1.631, -1.127, 0.067, 0.755, 1.367, 0.162, -0.072, 0.289, 2.388, 1.127, -0.706, 1.186, 0.815, -0.305, -1.001, -0.389, -0.871, 0.794, 0.5, 0.741, -0.348, -0.29, -0.924, 0.241, 0.16, -0.315, -0.149, -0.457, 0.616, -0.017, 0.386, 1.34, -0.311, -1.116, -1.706, -1.517, 0.781, 0.514, 1.126, -0.665, 0.583, -0.07, -0.192, -0.083, -0.624, 0.582, 0.502, 0.98, -0.39, 0.438, -0.023, -1.097, -1.149, 0.666, -0.831, -0.048, -1.257, 1.043, -1.676, -0.752, 1.964, -1.332, 0.057, -0.061, -0.858, -0.817, 0.92, -0.041, -0.364] point2 = [-0.625, -0.902, -0.869, 0.348, -1.461, 1.61, 0.34, 0.187, 0.232, -0.802, -0.666, 0.168, 0.898, 0.854, 1.668, -1.964, 0.745, -0.512, 0.034, 0.523, -1.01, -0.691, 1.542, 0.174, 1.026, 0.636, -0.185, -0.582, -3.384, 0.876, -0.418, 1.623, -0.224, 0.869, 1.38, -0.45, 0.021, 1.766, 0.915, -1.002, 1.464, 0.361, 0.407, -0.312, -0.623, 1.203, -0.776, 2.283, 0.73, 0.151, 0.393, -0.852, 1.286, 0.171, 0.306, 0.675, -0.283, -0.367, -0.556, -1.865, 1.194, 0.605, 1.309, -0.594, -0.715, -0.88, 1.115, -0.625, -1.915, -0.853, 0.489, -1.729, 1.105, -0.822, 0.13, 0.986, -0.459, 2.506, 0.997, 1.511, 0.412, 0.034, 0.109, 0.068, -0.267, -0.034, 1.614, -0.939, -0.06, -0.112, 0.026, -0.526, 0.608, 0.845, 0.424, 0.693, 0.209, -1.142, -0.666, 0.47] expected = 13.52044903100485 received = euclidean_distance(point1, point2) assert_equals(expected, received) print(&quot;test_euclidean_distance passed.&quot;) def test_get_closest_centroid(): &quot;&quot;&quot;Function for verifying if get_closest_centroid is correctly implemented. Will throw an error if it isn't. &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### # set up point1 = [0, 0, 0, 0] point2 = [1.1, 5.3, 55, -12.1] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [-10.1, 1, 23.2, 5.099]} assert_equals(&quot;centroid1&quot;, get_closest_centroid(point1, centroids_dict)) assert_equals(&quot;centroid2&quot;, get_closest_centroid(point2, centroids_dict)) point3 = [10.1, 1, 23.2, 5.099] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [10, 1, 23, 5], &quot;centroid3&quot;: [-100, 20.2, 52.9, -37.088]} assert_equals(&quot;centroid2&quot;, get_closest_centroid(point3, centroids_dict)) print(&quot;test_get_closest_centroid passed.&quot;) def test_update_assignment(): &quot;&quot;&quot;Function for verifying if update_assignment is correctly implemented. Will throw an error if it isn't. &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### # set up list_of_points, centroids_dict1, centroids_dict2 = setup_data_centroids() # centroids_dict1 received = update_assignment(list_of_points, centroids_dict1) expected = { &quot;centroid1&quot;: [[-1.36639346, -0.38664658, -1.02232584, -1.05902604], [1.13659605, -2.47109085, -0.83996912, -0.24579457], [-0.83095884, -1.73002213, -0.01361636, -0.3265274]], &quot;centroid2&quot;: [[-1.01714716, 0.95954521, 1.20493919, 0.34804443], [-1.48090019, -1.47491857, -0.6221167, 1.79055006], [-0.31237952, 0.73762417, 0.39042814, -1.1308523], [-0.78645408, 1.98342914, 0.31944446, -0.41656898], [-1.06190687, 0.34481172, -0.70359847, -0.27828666], [-2.01157677, 2.93965872, 0.32334723, -0.1659333], [-0.56669023, -0.06943413, 1.46053764, 0.01723844]] } assert_equals(expected, received) # centroids_dict2 received = update_assignment(list_of_points, centroids_dict2) expected = { &quot;centroid1&quot;: [[-1.36639346, -0.38664658, -1.02232584, -1.05902604], [1.13659605, -2.47109085, -0.83996912, -0.24579457], [-0.83095884, -1.73002213, -0.01361636, -0.3265274], [-1.01714716, 0.95954521, 1.20493919, 0.34804443], [-1.48090019, -1.47491857, -0.6221167, 1.79055006], [-0.31237952, 0.73762417, 0.39042814, -1.1308523], [-0.78645408, 1.98342914, 0.31944446, -0.41656898], [-1.06190687, 0.34481172, -0.70359847, -0.27828666], [-2.01157677, 2.93965872, 0.32334723, -0.1659333], [-0.56669023, -0.06943413, 1.46053764, 0.01723844]] } assert_equals(expected, received) print(&quot;test_update_assignment passed.&quot;) def test_mean_of_points(): &quot;&quot;&quot;Function for verifying if mean_of_points is correctly implemented. Will throw an error if it isn't. &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### # super simple list_of_points = [ [0, 0, 0, 0], [0, 0, 0, 0], ] assert_equals([0, 0, 0, 0], mean_of_points(list_of_points)) # a little more complicated list_of_points = [ [1, 2, 4, 6], [3, 4, 6, 8], ] assert_equals([2, 3, 5, 7], mean_of_points(list_of_points)) # negative list_of_points = [ [-1, -10, -70, -89], [2, 3, 55, 7], ] assert_equals([0.5, -3.5, -7.5, -41], mean_of_points(list_of_points)) # long version list_of_points = [[0.339, -0.65, 0.596, 0.804], [0.002, 0.973, -0.194, 0.016], [-0.121, -1.241, -0.69, 0.74], [-0.742, -0.033, -0.322, -0.536], [-0.434, -0.775, 0.943, -1.224], [-2.72, 0.955, -0.072, 0.392], [0.148, -0.939, 1.471, 1.217], [-0.226, 0.42, -0.687, 1.799], [-1.156, -0.69, 1.287, -0.984], [-0.625, 0.555, -0.025, -0.391]] expected = [-0.5535, -0.14250000000000002, 0.2307, 0.18330000000000002] received = mean_of_points(list_of_points) assert_equals(expected, received) print(&quot;test_mean_of_points passed.&quot;) def test_update_centroids(): &quot;&quot;&quot;Function for verifying if update_centroids is correctly implemented. Will throw an error if it isn't. &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### # set up list_of_points, centroids_dict1, centroids_dict2 = setup_data_centroids() # centroids_dict1 assignment_dict = update_assignment(list_of_points, centroids_dict1) expected = { 'centroid2': [-1.03386497, 0.774388037, 0.33899735, 0.023455955], 'centroid1': [-0.35358541, -1.529253186, -0.62530377, -0.543782673] } received = update_centroids(assignment_dict) assert_equals(expected, received) # centroids_dict2 assignment_dict = update_assignment(list_of_points, centroids_dict2) expected = { 'centroid1': [-0.82978110, 0.08329567, 0.04970701, -0.146715632] } received = update_centroids(assignment_dict) assert_equals(expected, received) print(&quot;test_update_centroids passed.&quot;) # main functions def main_test(): ####################################################### # You do not need to change anything in this function # ####################################################### test_euclidean_distance() test_get_closest_centroid() test_update_assignment() test_mean_of_points() test_update_centroids() print(&quot;all tests passed.&quot;) def main_2d(data, init_centroids): ####################################################### # You do not need to change anything in this function # ####################################################### centroids = init_centroids old_centroids = None step = 0 while not converged(centroids, old_centroids): # save old centroid old_centroids = centroids # new assignment assignment_dict = update_assignment(data, old_centroids) # update centroids centroids = update_centroids(assignment_dict) # plot centroid fig = plot_2d(assignment_dict, centroids) plt.title(f&quot;step{step}&quot;) fig.savefig(os.path.join(&quot;results&quot;, &quot;2D&quot;, f&quot;step{step}.png&quot;)) plt.clf() step += 1 print(f&quot;K-means converged after {step} steps.&quot;) return centroids def main_mnist(data, init_centroids): ####################################################### # You do not need to change anything in this function # ####################################################### centroids = init_centroids # plot initial centroids plot_centroids(centroids, &quot;init&quot;) old_centroids = None step = 0 while not converged(centroids, old_centroids): # save old centroid old_centroids = centroids # new assignment assignment_dict = update_assignment(data, old_centroids) # update centroids centroids = update_centroids(assignment_dict) step += 1 print(f&quot;K-means converged after {step} steps.&quot;) # plot final centroids plot_centroids(centroids, &quot;final&quot;) return centroids if __name__ == &quot;__main__&quot;: # main_test() data, label = read_data(&quot;data/mnist.csv&quot;) init_c = load_centroids(&quot;data/mnist_init_centroids.csv&quot;) final_c = main_mnist(data, init_c) write_centroids_tofile(&quot;mnist_final_centroids.csv&quot;, final_c) analysis from kmeans import get_closest_centroid from utils import load_centroids, read_data, assert_equals # ---------------------------------------------------------- # PROBLEMS FOR STUDENTS def update_assignment(list_of_points, labels, centroids_dict): &quot;&quot;&quot;Assign all data points to the closest centroids and keep track of their labels. The i-th point in &quot;data&quot; corresponds to the i-th label in &quot;labels&quot;. Arguments: list_of_points: a list of lists representing all data points labels: a list of ints representing all data labels labels[i] is the label of the point list_of_points[i] centroids_dict: a dictionary representing the centroids where the keys are strings (centroid names) and the values are lists of centroid locations Returns: a new dictionary whose keys are the centroids' key names and values are a list of labels of the data points that are assigned to that centroid. Example: Code: list_of_points = [[1.1, 1, 1, 0.5], [4, 3.14, 2, 1], [0, 0, 0, 0]] labels = [2, 1, 3] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [2, 2, 2, 2]} print(update_assignment(list_of_points, labels, centroids_dict)) Output: {'centroid1': [2, 3], 'centroid2': [1]} &quot;&quot;&quot; # creating a new dictionary dic = {} for i in list_of_points: # get the return of the function as key key = get_closest_centroid(i, centroids_dict) # give every dic[key] an empty list dic[key] = [] for i in list_of_points: # get the value in the labels with the certain index value = labels[list_of_points.index(i)] # get the key key = get_closest_centroid(i, centroids_dict) # give the value to the certain key dic[key].append(value) return dic def majority_count(labels): &quot;&quot;&quot;Return the count of the majority labels in the label list Arguments: labels: a list of labels Returns: the count of the majority labels in the list Example: Code: labels = [0, 3, 3, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 2, 2, 2] print(majority_count(labels)) Output: 6 &quot;&quot;&quot; dic = {} for i in labels: # initial the dic dic[i] = 0 for i in labels: # give the value to the dic if i in the dic.keys() let the # value of dic add one dic[i] += 1 max = 0 for i in list(dic.values()): if i &gt; max: max = i # assign the i to max we can get the max return max def accuracy(list_of_points, labels, centroids_dict): &quot;&quot;&quot;Calculate the accuracy of the algorithm. You should use update_assignment and majority_count (that you previously implemented) Arguments: list_of_points: a list of lists representing all data points labels: a list of ints representing all data labels labels[i] is the label of the point list_of_points[i] centroids_dict: a dictionary representing the centroids where the keys are strings (centroid names) and the values are lists of centroid locations Returns: a float representing the accuracy of the algorithm Example: Code: list_of_points = [[1.1, 1, 1, 0.5], [4, 3.14, 2, 1], [0, 0, 0, 0]] labels = [2, 1, 3] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [2, 2, 2, 2]} print(accuracy(list_of_points, labels, centroids_dict)) Output: 0.6666666666666666 &quot;&quot;&quot; sum = 0 v = update_assignment(list_of_points, labels, centroids_dict).values() for i in v: # get the sum of majority_count sum += majority_count(i) # get the value according to the definition return sum / len(labels) # ---------------------------------------------------------- # HELPER FUNCTIONS def setup_for_tests(): &quot;&quot;&quot;Creates are returns data for testing analysis methods. Returns: data, a list of data points labels, numeric labels for each data point centroids_dict1, three 4D centroids centroids_dict2, three non-random 4D centroids with poor starting values &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### list_of_points = [ [-1.01714716, 0.95954521, 1.20493919, 0.34804443], [-1.36639346, -0.38664658, -1.02232584, -1.05902604], [1.13659605, -2.47109085, -0.83996912, -0.24579457], [-1.48090019, -1.47491857, -0.6221167, 1.79055006], [-0.31237952, 0.73762417, 0.39042814, -1.1308523], [-0.83095884, -1.73002213, -0.01361636, -0.32652741], [-0.78645408, 1.98342914, 0.31944446, -0.41656898], [-1.06190687, 0.34481172, -0.70359847, -0.27828666], [-2.01157677, 2.93965872, 0.32334723, -0.1659333], [-0.56669023, -0.06943413, 1.46053764, 0.01723844] ] labels = [0, 1, 0, 2, 1, 2, 1, 2, 0, 0] centroids_dict1 = { &quot;centroid1&quot;: [0.1839742, -0.45809263, -1.91311585, -1.48341843], &quot;centroid2&quot;: [-0.71767545, 1.2309971, -1.00348728, -0.38204247], &quot;centroid3&quot;: [-1.71767545, 0.29971, 0.00328728, -0.38204247], } centroids_dict2 = { &quot;centroid1&quot;: [0.1839742, -0.45809263, -1.91311585, -1.48341843], &quot;centroid2&quot;: [10, 10, 10, 10], &quot;centroid3&quot;: [-10, 1, -10, 10], } return list_of_points, labels, centroids_dict1, centroids_dict2 # ---------------------------------------------------------- # TESTS def test_update_assignment(): ####################################################### # You do not need to change anything in this function # ####################################################### # set up (list_of_points, labels, centroids_dict1, centroids_dict2) = setup_for_tests() # test with centroids_dict1 answer = {'centroid3': [0, 1, 2, 1, 2, 2, 0], 'centroid1': [0], 'centroid2': [1, 0]} assert_equals( update_assignment(list_of_points, labels, centroids_dict1), answer) # test with centroids_dict2 answer = {'centroid1': [0, 1, 0, 2, 1, 2, 1, 2, 0, 0]} assert_equals( update_assignment(list_of_points, labels, centroids_dict2), answer) print(&quot;test_update_assignment passed&quot;) def test_majority_count(): ####################################################### # You do not need to change anything in this function # ####################################################### # single assert_equals(6, majority_count([0, 0, 0, 0, 0, 0])) assert_equals(5, majority_count([1, 0, 0, 0, 0, 0])) assert_equals(5, majority_count([0, 1, 1, 1, 1, 1])) # mixed assert_equals(4, majority_count([0, 0, 1, 1, 0, 0])) assert_equals(4, majority_count([0, 2, 2, 2, 3, 3, 0, 1, 1, 0, 0])) # tied max count assert_equals(4, majority_count([0, 2, 2, 2, 0, 2, 0, 0])) print(&quot;test_majority_count passed&quot;) def test_accuracy(): ####################################################### # You do not need to change anything in this function # ####################################################### # set up (list_of_points, labels, centroids_dict1, centroids_dict2) = setup_for_tests() # test with centroids_dict1 expected = 0.5 received = accuracy(list_of_points, labels, centroids_dict1) assert_equals(expected, received) # test with centroids_dict2 expected = 0.4 received = accuracy(list_of_points, labels, centroids_dict2) assert_equals(expected, received) print(&quot;test_accuracy passed&quot;) def main_test(): ####################################################### # You do not need to change anything in this function # ####################################################### test_update_assignment() test_majority_count() test_accuracy() print(&quot;all tests passed.&quot;) if __name__ == &quot;__main__&quot;: centroids = load_centroids(&quot;mnist_final_centroids.csv&quot;) # Consider exploring the centroids data here # Uncomment the line below for Part 2 Step 2, 3, and 4: # main_test() data, label = read_data(&quot;data/mnist.csv&quot;) print(accuracy(data, label, centroids)) ","link":"https://irisdin.github.io/post/SEbRoAqB2/"},{"title":"Should employers be allowed to surveil their employees' or potential employeeâ€™s digital actions and made decisions?","content":" Social media provided a platform which people can share and post anything related to their daily lives. In 2020, there are over 3.6 billion people who are using social media (Tankovska, 2021) and the number are still increasing with the development of our society. At the same time, more and more employers and human resources mangers start to use the social media to trace the candidates and their employees. According to a survey, 60% percent of the employers uses social media to trace their potential candidates and employees. (â€œNumber of Employers Using Social Media to Screen Candidates Has Increased 500 Percent over the Last Decadeâ€,2016) Also, many employees are fired because of certain post they spread on the social media. However, should the employers have the right to monitor their employees' or potential employeeâ€™s social media and make decisions? The three primary groups who concerned about this problem are the current/potential employees and employer. As many employers are tracing the social media and supporting the usage of surveillance on social media, many employees argued that this behavior invaded their privacy and also leads to some personal bias that unrelated to the working abilities. From my perspective, it is necessary and important to trace their digital actions for several reasons: allowing employer to trace the social media helps to protect the profits of the company and social media serves a great role in selecting the candidates. To begin with, monitoring employeeâ€™s social media helps to protect the overall benefits of the company. In the workplace, majority of workers are being affected by heavy work tasks, high intensity working environment, and increasing work pressure. Under that circumstances, many workers choose to express their emotions and dissatisfaction with work on social platforms. These posts and some mis-behavior on the Internet will influence the company â€˜s reputation directly. Moreover, lots of employees post something that should not be posted online. Contents such as product information, proprietary information, company information or other sensitive data may be unintentionally or intentionally leaked through social media channels. Sometimes, these post on the social media will lead to some legal actions which cost lots of time and effort of a company. Here is an example. In the case of Tianmai Juyan Education Co., Ltd. v. Ma, which is a labor dispute, (â€œTianmai Juyuan (Beijing) Education Technology Co., Ltd. and Ma Weiâ€™s first-instance civil judgmentâ€, 2018) Ma was angry because of the companyâ€™s action of salary reduction. Thus, he posted a screenshot of the companyâ€™s private incentive plan and the companyâ€™s internal salary reduction notice in a Chinese social media called WeChat. The Second Intermediate People's Court of Beijing proposes that after Ma posted information related to the company's internal affairs to WeChat, people around him saw the post and left some comments within the social media. As a result, his behavior caused the dissemination and leakage of the company's internal business information. Regardless of whether the information posted is a trade secret of the company, Ma, as an employee, should not arbitrarily post company internal information to social media due to his personal unsatisfaction. His behavior obviously violates the employee's duty of loyalty and is an excessive expression of wage reduction. The social media contains huge quantity of people, and one post could be spread soon to tremendous amount of people and ruined a company. The monitoring of employeeâ€™s social media like Facebook or Instagram can help the company to find and solve some potential problems quicky. Also, according to the Federal Rules of Civil Procedure (FRCP), all work-related content posted by employees on social media must be visible, regardless of whether the content is created through a personal account. Therefore, the company could reach out and deal some inappropriate post as soon as possible. Another aspect is that social media is a great tool for the human resources manager to select the candidates besides the resume they provided. From the resume, the employer can see your abilities and official grades while the personal profile just indicated your personality. Through the research of more than five hundred college students and their Facebook homepages, (knowledgeatwharton,2012) the researchers found the college studentsâ€™ &quot;Facebook&quot; personal homepage displayed the so-called &quot;Five Personality Traits&quot;:Easy-going, Consciousness, Extroversion, Emotional Stability and Openness. After communicating with the employers of some of these students, the researchers found out that the students' working performance is closely related to the two personality traits: emotional stability and easygoing. Combing the resume and social media can help to draw a real personal profile of a potential candidate. And the top three social media that the most employers are likely to check are: LinkedIn, Facebook, and Twitter. (â€œSocial media Screenings gain in popularity, 2020â€) Besides personality, it is also important to check the candidatesâ€™ online persona to make sure they provided a positive image. For example, they will filter out some candidates with misbehavior: some people lie about their qualification, some posts linked to criminal actions, and discriminative speech against other people. Since misbehaviors would cause serious consequences in the future and lead to more troubles. Meanwhile, even seemingly innocuous details and behaviors, such as using unprofessional terms or publishing too frequently, can also help to judge the drawbacks of the candidates. Moreover, through the combination of algorithms and social media, employers can determine whether the candidate has a strong interest in their company and position. For example, if an applicant browses the company's official website records frequently, looks at position-related articles, or pays attention to the development of the company, the applicants will leave a better impression to the employer. Finally, social media can be a good monitoring station for psychological risks. The employers are able to extract and analyze psychological risks of one candidate from the database of emotional catharsis. For the positive position, there are also some counterclaims that argue against it. One opinion suggested that the company rejects qualified job applicants just because they do not like what they find online. To be more specific, some manger will choose the candidate based on their own preference. Some candidates might lose their just because their race, sexual orientation, or some wrong post they made. Many people thought it is an unfair selection and the surveillance of social media should stop. Also, by looking at the social media webpage of a person, it is easy to form a stereotype of a person while the fact is not like that. For example, if a person posted a picture of drinking, that normally leave bad first impression to the employer. Some employers will subjectively believe that the competitor likes to drink and form the stereotypes of this person. The employers might make the decision of rejection just because he or she thought drink might affect the work in the future. However, the fact would be this competitor just drank alcohol due to a bad mood and did not have the habit of drinking regularly. The way of using social media to measure a person is unfair and also leading to some issues. Thus, employers should not trace the candidatesâ€™ personal social media. For this point, I want to make a rebuttal. The behavior of rejecting people just because they post the thing the employer did not like will also influence and bring harmful consequences to the employerâ€™s career. The main mission for the employer is to find a person that really fit certain position and meet the core value of the companies. To find the right candidate, the employer needs to use more comprehensive of measurements (â€œHow to hire the right personâ€) according to the New York Times. Employers simply use social media as an auxiliary tool to measure competitors. A professionally qualified employer will make an opinion based on the competitor's comprehensive ability and whether he or she is competent for certain specific position. Making decisions based solely on the content of social media is sloppy, and this action will cause the company to lose some potential talent so many employers will not do that. Another aspect we cannot overlook is that social media is still a great tool to reflect the characters of competitors, (Seidman, 2020) and we cannot ignore the importance of this tool. When employees are deciding on two competitors with the same ability, personality may become the main factor determining who will enter the company. In addition, employers are aware of potential legal and regulatory risks. If the hire manager learn that an applicant is a homosexual or other disadvantaged group and hire someone else just because they saw that information on the social media, the manager may face discrimination prosecution. Employers will not take huge risks to make decisions based solely on their personal preferences for social media. As a result, social media can be used as an objective and rational tool to measure the competence of competitors. In general, this problem is still very controversial and involved with many ethnical and technological issues. From all the points that I listed above, I agree with the point that employers should be allowed to surveil their employees' or potential employeeâ€™s digital actions and made decisions. Works Cited: How to hire the right person. (n.d.). Retrieved February 22, 2021, from https://www.nytimes.com/guides/business/how-to-hire-the-right-person K. (n.d.). Is it appropriate to assess the job applicantâ€™s social network performance? Retrieved February 22, 2021, from http://www.knowledgeatwharton.com.cn/article/3087/ Number of employers using social media to screen candidates has increased 500 percent over the last decade. (n.d.). Retrieved February 22, 2021, from http://press.careerbuilder.com/2016-04-27-Number-of-Employers-Using-Social-Media-to-Screen-Candidates-Has-Increased-500-Percent-over-the-Last-Decade Seidman, G. (2020, September 21). What can we learn about people from their social media? Retrieved February 22, 2021, from https://www.psychologytoday.com/us/blog/close-encounters/202009/what-can-we-learn-about-people-their-social-media#:~:text=The%20content%20on%20social%20media%20predicts%20personality.&amp;text=Bachrach%20and%20colleagues%20found%20they,is%20related%20to%20their%20personality. Sheri B. Cataldo &amp; Howard S. Weisel. (n.d.). Social media Discovery 101 #federal Courts #isdiscoverypermitted? Retrieved February 23, 2021, from https://litigationcommentary.org/2016/2016-july-august/1248-social-media-discovery-101-federal-courts-isdiscoverypermitted Social media Screenings gain in popularity. (n.d.). Retrieved February 22, 2021, from https://www.businessnewsdaily.com/2377-social-media-hiring.html Tankovska, H. (2021, January 28). Number of social media Users 2025. Retrieved February 22, 2021, from https://www.statista.com/statistics/278414/number-of-worldwide-social-network-users/#:~:text=Social%20media%20usage%20is%20one,almost%204.41%20billion%20in%202025. Tianmai Juyuan (Beijing) Education Technology Co., Ltd. and Ma Weiâ€™s first-instance civil judgment. (n.d.). Retrieved February 22, 2021, from http://www.sq142.com/m/view.php?aid=302 ","link":"https://irisdin.github.io/post/da1sPo1x1/"},{"title":"Analyzing the problems and design of Airbnb interfaces","content":" About the Product: Airbnb is a platform that people can rent their own houses to others. And guests can find the commendations like their own house. A lot of people choose Airbnb since it is normally cheaper than a hotel room and there are lots of options for choosing. People can also earn extra money by renting the house out. The platform gets profits by charging the commission from the costumerâ€™s booking. Currently, people can access Airbnb via its website and mobile app. Stakeholder Research: Direct Stakeholders: The people who booked their house would be the direct stakeholder group for this design. In definition, stakeholders are people who have an interest in the companyâ€™s affairs. The tenant wants to find a comfortable place to stay or for other purposes. As a result, this group would be the direct stakeholder. In order to make analysis deeply, I used the research method like surveys and review of online discuss. I choose to survey since there are lots of friends around me are using the Airbnb and I can get lots of real user experience by taking the survey. I designed the questionnaire to ask 30 people who have used Airbnb their satisfaction and concerns about this platform. Moreover, I reviewed the online discussion of Airbnb via the platform like Reddit and App-store. I choose this method since it is easy to access. I look at the userâ€™s rating and their reason for rating specifically. From my research, the stakeholders list lots of suggestions. Their primary motivation for tenant was to find a cheap or a big house according to their needs. Their goal is gaining benefits through the platform. The values they want to gain is a positive cycle for both the renters and tenants. However, the design did not fit their primary motivations, goals, and values. The researched reported many problems: the filter cannot help to find a good house effectively, people cannot comment with videos or pictures, and there are language barriers that may confuse the guests. Indirect Stakeholders: The indirect stakeholders would be the neighborhoods of those renters. They are the indirect stakeholders since the majority of houses that used for Airbnb are private houses. Unlike the hotel, majority of the private living will have a group of neighbors. The design of inaccurate map would make the tenant to knock on neighbor house which disturb their lives. Design Critique : The overall userâ€™s experience and their attitude on the interaction design is good according to the survey and online discussion. And we can see people with various abilities are using Airbnb. However, the platform still contains some problems related to the useâ€™s interfaces and interaction design. To be specific, the filter function cannot help the stakeholders(tenant) to find a ideal house effectively. According to the UI interfaces design principles, it fails to achieve the function of clarity. Moreover, after we selected the place and date in the primary page. The next interfaces that pop up will cause confusions for people with its poor design in visibility and feedback. Problem 1 Label: The first problem is related to the product filter. The design page fails certain categories of UI design principles. One primary principle of UI design is making users comfortable to interact with the design. As the picture shown, the advertisement that listed at the top of page will make the tenant feel uncomfortable. Also, the primary motivation for the tenant is to find a house according to their own need and interests. The filter recommended lots of assumed options for guests and people need to scroll down almost three pages to find some crucial factors that the price. The design is unnecessary and waste peopleâ€™s time. Another principle for UI design is to reduce the cognitive loads for those stakeholders. The designer should reduce the actions required for achieving the tenantâ€™s goal and extract the most useful information on one page. However, there are too-much information on the filter page and the users need to click lots of times to find a suitable place. Moreover, there is also an interaction design problem. The feedback for the price range is low. The tenant cannot get a good view and feedback about the price they selected through the faded region. Problem 2 Label: The second problem is on the interfaces that displayed house information after the stakeholders clicked the destination and data for travel. According to the UI design principle, the page fails to create an interface that palace users in control. Since when we entered a foreign destination, the house information will translate to the local language and there is no place for translation. For tenant that lived in another country, they will feel confuse about the overall page. They did not what the meaning of those words. And the visibility of the bottom filter is low so some people will ignore that function and the only thing they can get is the picture and price. However, the large portion of the image did not indicate so much useful information. People need to click inside to see the detail information which lead to the cognitive reload. Lastly, the page provides some misleading information. The price showed on the page is not the real price for the house. The renter often adds some extra fees, and the tenant would only know until they reached the final payment page. Redesign Proposal: For the redesigned model, I tried to fix the second problem that stated above. To begin with, the original visibility for filter is poor. The redesign added color with clear word hint for the stakeholders to see the function and select a better house according to their own needs. Also, the redesign model helps to create an interface that easy for people to navigate according to the UI design principle. For the language barrier issue, people can use the translation bottom. Besides, the redesign model put three house information on one page. That saves the stakeholderâ€™s time and also helps them to make compare. The stars make the user directly to see others attitude towards the house on the first page which reduce the cognitive load. For the misleading information price, the new model corrects the price into the average price for the final payment each tenant made before. There are still some limitations with this new design. The first page still did not provide efficient information for tenant to choose a good house. One possible solution could be introducing a sorting bottom that can help to sort the house from the highest price to lowest price, etc. Otherwise, there could be some variation of my design idea. The stars can be transformed into the percentage of user satisfaction. Another variation could be changing the three pictures per page into two pictures per page and add more information about the house. Idea 1 - Redesign of Issue: As we can see on the graph, the redesign of the models involved some changes. The new model includes increasing visibility for the function â€œfilterâ€, the new translation button, and the stars. Also, the new model put more pictures of houses per page and fix the issue of price. The change of filter is based on the principles of interaction design. Based on Don Normanâ€™s idea, â€œthe more visible the element is, the more likely users will know about them and use themâ€. Filter is a powerful function that can help people to find an ideal house based on their needs. All other change is based on the principles of UI design, all those changes are devoted to make a user-based interfaces that easy and comfortable to navigate. References: Airbnb (n.d.). Community Center. Retrieved February 01, 2021, from https://community.withairbnb.com/t5/Community-Center/ct-p/community-center (n.d.). Retrieved February 01, 2021, from https://uxdesign.cc/airbnb-redesigning-for-the-new-normal-66fb273de769 Rekhi, S. (2018, February 25). Don Norman's Principles of Interaction Design. Retrieved February 01, 2021, from https://medium.com/@sachinrekhi/don-normans-principles-of-interaction-design-51025a2c0f33 The Basic Principles of User Interface Design. (2020, November 27). Retrieved February 01, 2021, from https://www.uxpin.com/studio/blog/ui-design-principles/ Appendix: ","link":"https://irisdin.github.io/post/59JrCTeTC78/"},{"title":"Logistic Regression Model in U.S. College Admission Result Forecast","content":" Abstract: Logistic Regression is a type of classification algorithm involving a linear discriminant. Logistic Regression model measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logit/sigmoid function. Unlike linear regression, logistic regression does not try to predict the value of a numeric variable given a set of inputs. Instead, the output is a probability that the given input point belongs to a certain class. With the help of the logistic regression model, in this paper, we will create a program to predict the admission decision for U.S. graduate school of Chinese applicant. Input And Output Data Information: Data Processing: When we collect data, outliers may appear. Unlike support vector machine (SVM), Logistic Regression result will be heavily impacted by the outliers (as all data points contribute). If any independent variable in the regression model involves the wrong data, the probability can change hugely. For example, we dropped all GRE total score lower than 250 and Verbal score lower than 120 since they are unlikely to happen in reality and probably out of data entry error Outlier Detection &amp; Processing: Null/Missing Value Imputi Turning Categorical Variables into Numerical Value/One-hot Encod Dealing with Specific Issues in the Datas Materials and Methodology: Libraries like pandas, numpy and pyplot are quite standard python data processing/plottinglibraries that we use for basic data manipulation. Additionally, for this model, we importedthe sklearn library, which is a powerful package that contains our main algorithm of logisticregression. Also, packages like preprocessing and metrics in sklearn can help us preprocessdata and evaluate the performance of the model. For instance, the confusion matrix and itsderived metrics would be used to visualize and evaluate the model performance. Outlier Detection &amp; Processing: SMOTE to Handle Data Imbalance Recursive Feature Elimination (RFE) Model Summary Table &amp; Explanation Results and performance: Link to the paper/ code / dataset: â¬‡ï¸ Github ","link":"https://irisdin.github.io/post/projectlogistic/"},{"title":"Informatics major application share :)","content":" Basic information:ğŸ˜€ Cumulative GPA: 3.88 Pre-requsite GPA: 3.90 Course Taken:ğŸ“ CSE 160 CSE 180 INFO 200 STAT 311 ... Info related experience/skills ğŸ”§: ğŸ¤˜Codingï¼špython, java, R ğŸ•¸ï¸web developmentï¼šhtml5, Javascript ğŸ‘©â€ğŸ¨Visual designï¼šAutocad, Figma, Balsamiq ğŸ‘©â€ğŸ’»Data analysisï¼šPowerBIï¼ŒTableauï¼ŒMicrosoft Excel âœï¸Editorï¼šVscode ,Eclipse You could check my projects on: Linkedin Github Application essay: ğŸŒŸMajor component: Experiences with information Experiences with diversity, equity, and inclusion. Goals after college. Learning skills. Writing My decision to study informatics derives from my ambition to serve human beings through technology, but coming to that decision was a process of exploration. I was first exposed to contents related to informatics in My senior year in high school. In the beginning of Covid-19 in 2020, all of us was panic due to lack of information about the virus. I searched all news and media to put the pieces together and caught the sight of whole puzzle. That was when I realized the importance of information. An efficient information system could help me turn information into actionable knowledge. Therefore, I have a strong intent to develop solutions to the world's information challenges for myself and the communities. This is my first love into informatics. After that, in the university, I have a deeper understanding of informatics. As a member of Chinese students who account for the vast majority of American international students, Iâ€™m much better able to realize that international students are facing many serious social, physical and spiritual problems. Especially during the pandemic period, many students can only take online classes, so time equation, lacking of communication with teachers and other questions cause emotional problems and academic pressure. In communicating with classmates, I found that many are under this kind of pressure silently, some even suspend or drop out, including myself. Furthermore, I found that not only Chinese students, but students from other countries also face the same psychological problems. Thus, I had the idea of designing an app to help solve the psychological problems of students. I would like to design a platform to help this special group in our community. By using machine learning methods, I could collect dataï¼Œprovide them information from the school, different solutions for their concerns and match different friends or mentors to help them. So here comes Mental Bridge. Our team for Info200 build the simple framework of the app together. But that was not enough, I would like to make the platform and help our international community in real world. Through three semesters' university life, I am more determined in learning informatics. The past learning experience has given me enough confidence and ability to deal with future tasks. I took a glimpse of the whole information world though Info200. I learned fundamental skills of manipulating data, machine learning and making simple regression in Info180. By using right ways to analyze data through the models in this world which has tons of information, I could push right news and right people to help my Mental Bridge users. Therefore, interest and reality motivated me to make the determination to study informatics. In my opinion, diversified courses in major informatics will make me be able to learn foundational skills and practical technology in information technology, also provide me with an interdisciplinary perspective to close the gap between technology and human needs by translating usersâ€™ requirements into technological solutions. The course Data Reasoning in a Digital World can teach me analyze the authenticity of information sources, on the basis of true information. The course Foundational Skills for Data Science can teach me how to analyze and use data through programming. While Databases and Data Modeling introduces the relevant knowledge of the database system, and links data modeling decisions to social justice outcomes. Whatâ€™s more, my serious learning attitude and hardworking spirit will also help me to my future study of informatics. I believe that after learning more knowledge, I can continuously optimize and develop Mental Bridge, so that it can collect studentsâ€™ psychological problems and provide suitable solutions. In other words, I want to finally create a bridge connecting students and counselors to solve the current situation when seeking help. And through my study in Informatics, I could make the program more accurate and eliminate the bias of misunderstanding users' needs in the progress of matching right solutions for every single student who needs help not only Chinese students. We are living in a era of major technological advances and information exploration. I will dedicate myself in studying how to use these information to improve our lives and make our community better. Through informatics major, I will explore the methods of change our community or even our world. all right reserved/any form of plagiarism is not allowed ","link":"https://irisdin.github.io/post/info/"},{"title":"About","content":" weclcome to my website:) ğŸ  About the site This is a personal website that generate my work, thought and project. * remember to click to read the full content ğŸ‘¨â€ğŸ’» About me ISFJ-A From Ningbo Chinaâ¡ï¸Seattle US The barstow school 20' University of washington informatics 24' : data science &amp; human computer interacton track A code writer/data science Enthusiast/ future educator :) ğŸ“¬ contact information email address: &lt;irisding20020213@outlook.com&gt; follow me on: instagram: @iris_oovo feel free to contatct me if you have any questions. ","link":"https://irisdin.github.io/post/about/"}]}