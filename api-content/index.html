{"posts":[{"title":"# 好书推荐 无限近似于透明的蓝","content":" 《无限近似于透明的蓝》是24岁的村上龙笔下的第一部作品。故事的背景发生在二十世纪七十年代左右，那时的日本正在飞速的发展，经济和人们的物质水平也得到了前所未有的提高。但在东京福生区美军基地附近，有一群浑浑噩噩的青年男女整日纵情放任，没有约束，对自己的未来感到迷茫又恐惧。 文章借用了19男子“龙”的视角，直白的将死亡, 性爱, 酒精，迷幻剂，腐烂的食物，毒品...描述在书中。“龙”就像作者村上龙的缩影，在黑暗的深渊中尝试着救赎自己，但却越陷越深。龙时常也在怀疑，那个照顾同伴，热爱美好风景的自己是不是逐渐被身边糟糕的环境打败了。最后的他选择了死亡的方式来救赎自己的灵魂。龙躺在草地上，幻想着无限近似于蓝色的天空，结束了自己的生命。 真实细节的描写，这种肮脏的美学用文字的方式给读者带来强烈的视觉冲击和不适感。这种真诚直白的写作方式无疑对当时的文学流派带来了冲击，展现了当时社会最真实的颓废和混沌，也看不到一丝的喜悦。书中描绘的场景和主人公的理念毫无疑问的说不符合一般人们“正确”的价值观。 这本带着忧郁色彩的青春文学在日本文学中留下了自己独特的色彩，将亚文化主题的作品送上舞台，展现了那个年代”透明族“最真实的生活状态... ","link":"https://irisdin.github.io/post/BWIR5TCy3/"},{"title":"如何用Github pages轻松搭建属于自己的个人网站😉","content":"Github page是什么❓ GitHub Pages是GitHub提供的一个网页寄存服务，可以用于存放静态网页，包括博客、项目文档，甚至整本书。一般GitHub Pages的网站使用github.io的子域名。本站点 https:/irisdin.github.io/ 就是通过 Github pages进行搭建完成✅ （来源：维基百科） 我为什么选择Github page❓ 无需购买云服务器 免费 没有繁琐的环境配置以及系统搭建 适合静态博客的搭建 不足之处：😢 基于Git，需要了解一定的git指令和基础的编程知识 项目和网站的大小不能超过1GB 每小时不得超过 10 个更新的网站版本 每个月的也要注意带宽使用上限为 100GB 如何使用Github pages 来搭建自己的网站 注册一个Github账号 官网链接 Github 在个人主界面里新建一个 Repository 在 new repository填写域名（不可修改！）格式为：username.GitHub.io 例如我的用户名是IrisDin 那则填写IrisDin.github.io 创建完之后点进我们的repository 点击setting 在左边的目录当中找到pages并进入Github pages，选择一个自己喜欢的主题 ** 最后一步 选择完你喜欢的主题之后 GitHub Pages 会自动生成网站。点击 Commit changes 按钮，你的网站就上线啦😃 在浏览器里输入你的域名（repository的名字），例如 Irisdin.GitHub.io，你就可以看到你的网站在网页上的效果了。 使用第三方的静态模板系统来自定义自己的网站 github官方提供了有限的模版，我们也可以使用第三方的静态模版系统来使自己的网站变的更加美观，例如使用Node.js 编写的 Hexo，Go 编写的 Hugo和Python 编写的 Pelican。也可以在github中多浏览找到自己喜欢的模版：） 如何配置自定义域名 如果你不喜欢github.io这个后缀的话，你可以轻松的配置自己喜欢的域名 Github page自定义配置域名官方文档 GitHub Pages 提供免费为自定义域名开启 HTTPS 的功能，不需要自己提供证书，只需要将自己的域名使用 CNAME 的方式指向自己的 GitHub Pages 域名即可。（没有自己的域名的话可以在阿里云或者腾讯云上购买） 在域名管理中添加两条记录类型为CNAME的解析，一条主机记录为@，一条主机记录为www，格式为http://xxxx.github.io 在本地hexo文件的source文件夹下创建一个CNAME文件，编辑CNAME文件，填写域名，例如http://xxxx.cn 在git page 中设置中勾选https 等待更新即可 ","link":"https://irisdin.github.io/post/-3Hrfy0Up/"},{"title":"Social networking and recommendation sysytems practice","content":"readme link🔻 uw cse only for practice purpose/plagiarism is not allowed🔻 cse misconduct guideline programming language : python import utils # noqa: F401, do not remove if using a Mac import networkx as nx import matplotlib.pyplot as plt from operator import itemgetter &quot;&quot;&quot;Builds and returns the practice graph &quot;&quot;&quot; def get_practice_graph(): practice_graph = nx.Graph() practice_graph.add_node(&quot;A&quot;) practice_graph.add_node(&quot;B&quot;) practice_graph.add_node(&quot;C&quot;) practice_graph.add_node(&quot;D&quot;) practice_graph.add_node(&quot;E&quot;) practice_graph.add_node(&quot;F&quot;) practice_graph.add_edge(&quot;A&quot;, &quot;B&quot;) practice_graph.add_edge(&quot;A&quot;, &quot;C&quot;) practice_graph.add_edge(&quot;B&quot;, &quot;C&quot;) practice_graph.add_edge(&quot;B&quot;, &quot;D&quot;) practice_graph.add_edge(&quot;C&quot;, &quot;D&quot;) practice_graph.add_edge(&quot;C&quot;, &quot;F&quot;) practice_graph.add_edge(&quot;D&quot;, &quot;F&quot;) practice_graph.add_edge(&quot;D&quot;, &quot;E&quot;) return practice_graph def draw_practice_graph(graph): &quot;&quot;&quot;Draw practice_graph to the screen. &quot;&quot;&quot; nx.draw_networkx(graph) plt.show() def get_romeo_and_juliet_graph(): &quot;&quot;&quot;Builds and returns the romeo and juliet graph &quot;&quot;&quot; rj = nx.Graph() rj.add_nodes_from(['Paris', 'Mercutio', 'Escalus', 'Capulet']) rj.add_nodes_from(['Juliet', 'Tybalt', 'Nurse']) rj.add_nodes_from(['Montague', 'Benvolio', 'Romeo', 'Friar Laurence']) rj.add_edge('Romeo', 'Friar Laurence') rj.add_edge('Romeo', 'Benvolio') rj.add_edge('Romeo', 'Montague') rj.add_edge('Romeo', 'Mercutio') rj.add_edge('Montague', 'Benvolio') rj.add_edge('Montague', 'Escalus') rj.add_edge('Juliet', 'Tybalt') rj.add_edge('Juliet', 'Capulet') rj.add_edge('Juliet', 'Friar Laurence') rj.add_edge('Juliet', 'Romeo') rj.add_edge('Tybalt', 'Capulet') rj.add_edge('Escalus', 'Mercutio') rj.add_edge('Escalus', 'Paris') rj.add_edge('Paris', 'Mercutio') rj.add_edge('Nurse', 'Juliet') rj.add_edge('Capulet', 'Escalus') rj.add_edge('Capulet', 'Paris') return rj def draw_rj(graph): &quot;&quot;&quot;Draw the rj graph to the screen and to a file. &quot;&quot;&quot; nx.draw_networkx(graph) plt.savefig(&quot;romeo-and-juliet.pdf&quot;) plt.show() ### # Problem 2 ### def friends(graph, user): &quot;&quot;&quot;Returns a set of the friends of the given user, in the given graph. &quot;&quot;&quot; # This function has already been implemented for you. # You do not need to add any more code to this (short!) function. return set(graph.neighbors(user)) def friends_of_friends(graph, user): &quot;&quot;&quot;Find and return the friends of friends of the given user. Arguments: graph: the graph object that contains the user and others user: a string Returns: a set containing the names of all of the friends of friends of the user. The set should not contain the user itself or their immediate friends. &quot;&quot;&quot; # creating a new set set_frined = set() # finding frineds for a given user for friend in graph.neighbors(user): # reviewing each specific person for people in graph.neighbors(friend): if people != user: if people not in graph.neighbors(user): if people not in set_frined: set_frined.add(people) return set_frined def common_friends(graph, user1, user2): &quot;&quot;&quot;Finds and returns the set of friends that user1 and user2 have in common. Arguments: graph: the graph object that contains the users user1: a string representing one user user2: a string representing another user Returns: a set containing the friends user1 and user2 have in common &quot;&quot;&quot; # return a set that contains the coomon user return friends(graph, user1) &amp; friends(graph, user2) def number_of_common_friends_map(graph, user): &quot;&quot;&quot;Returns a map (a dictionary), mapping a person to the number of friends that person has in common with the given user. The map keys are the people who have at least one friend in common with the given user, and are neither the given user nor one of the given user's friends. Example: a graph called my_graph and user &quot;X&quot; Here is what is relevant about my_graph: - &quot;X&quot; and &quot;Y&quot; have two friends in common - &quot;X&quot; and &quot;Z&quot; have one friend in common - &quot;X&quot; and &quot;W&quot; have one friend in common - &quot;X&quot; and &quot;V&quot; have no friends in common - &quot;X&quot; is friends with &quot;W&quot; (but not with &quot;Y&quot; or &quot;Z&quot;) Here is what should be returned: number_of_common_friends_map(my_graph, &quot;X&quot;) =&gt; { 'Y':2, 'Z':1 } Arguments: graph: the graph object that contains the user and others user: a string Returns: a dictionary mapping each person to the number of (non-zero) friends they have in common with the user &quot;&quot;&quot; # creating a new dictionary common_map = {} for friend in friends_of_friends(graph, user): # len can not be negative if len(common_friends(graph, user, friend)) &gt; 0: common_map[friend] = len(common_friends(graph, user, friend)) return common_map def number_map_to_sorted_list(map_with_number_vals): &quot;&quot;&quot;Given a dictionary, return a list of the keys in the dictionary. The keys are sorted by the number value they map to, from greatest number down to smallest number. When two keys map to the same number value, the keys are sorted by their natural sort order for whatever type the key is, from least to greatest. Arguments: map_with_number_vals: a dictionary whose values are numbers Returns: a list of keys, sorted by the values in map_with_number_vals &quot;&quot;&quot; # sort both of the key through itemgetter function dict_item = map_with_number_vals.items() sort = sorted(sorted(dict_item, key=itemgetter(0)), key=itemgetter(1), reverse=True) # creating a new list sorted_new_list = [] for item in sort: sorted_new_list.append(item[0]) return sorted_new_list def recommend_by_number_of_common_friends(graph, user): &quot;&quot;&quot; Returns a list of friend recommendations for the user, sorted by number of friends in common. Arguments: graph: the graph object that contains the user and others user: a string Returns: A list of friend recommendations for the given user. The friend recommendation list consists of names/IDs of people in the graph who are not yet a friend of the given user. The order of the list is determined by the number of common friends (people with the most common friends are listed first). In the case of a tie in number of common friends, the names/IDs are sorted by their natural sort order, from least to greatest. &quot;&quot;&quot; common_list = number_of_common_friends_map(graph, user) sorted_friend_list = number_map_to_sorted_list(common_list) return sorted_friend_list ### # Problem 3 ### def influence_map(graph, user): &quot;&quot;&quot;Returns a map (a dictionary) mapping from each person to their influence score, with respect to the given user. The map only contains people who have at least one friend in common with the given user and are neither the user nor one of the users's friends. See the assignment writeup for the definition of influence scores. &quot;&quot;&quot; # creating a new dictionary new_map = {} for name in friends_of_friends(graph, user): score = 0 for person in common_friends(graph, user, name): # calculating the score based on the definition score = score + 1/len(friends(graph, person)) new_map[name] = score return new_map def recommend_by_influence(graph, user): &quot;&quot;&quot;Return a list of friend recommendations for the given user. The friend recommendation list consists of names/IDs of people in the graph who are not yet a friend of the given user. The order of the list is determined by the influence score (people with the biggest influence score are listed first). In the case of a tie in influence score, the names/IDs are sorted by their natural sort order, from least to greatest. &quot;&quot;&quot; new_list = influence_map(graph, user) return number_map_to_sorted_list(new_list) ### # Problem 5 ### def get_facebook_graph(): &quot;&quot;&quot;Builds and returns the facebook graph &quot;&quot;&quot; # creating a graph named facebook facebook = nx.Graph() # reading the file with open('facebook-links.txt') as file: for line in file: line = line.split(' ') facebook.add_edge(int(line[0]), int(line[1])) return facebook def main(): practice_graph = get_practice_graph() # Comment out this line after you have visually verified your practice # graph. # Otherwise, the picture will pop up every time that you run your program. draw_practice_graph(practice_graph) rj = get_romeo_and_juliet_graph() # Comment out this line after you have visually verified your rj graph and # created your PDF file. # Otherwise, the picture will pop up every time that you run your program. draw_rj(rj) ### # Problem 4 ### print(&quot;Problem 4:&quot;) print() # unchanged recoomendation list list_unchange = [] # changed recoomendation list list_change = [] # comparing and finding the difference for people in rj.nodes(): influence_algorithim = recommend_by_influence(rj, people) common_algorithim = recommend_by_number_of_common_friends(rj, people) if influence_algorithim == common_algorithim: list_unchange.append(people) else: list_change.append(people) print(&quot;Unchanged Recommendation:&quot;, sorted(list_unchange)) print(&quot;Changed Recommendation:&quot;, sorted(list_change)) ### # Problem 5 ### facebook = get_facebook_graph() # assert len(facebook.nodes()) == 63731 # assert len(facebook.edges()) == 817090 ### # Problem 6 ### print() print(&quot;Problem 6:&quot;) print() # Get user id that is mutiple 1000 list. user_mutiple_1000 = [] # finding all users that is the mutiple of 1000 in the dataset for people in list(facebook.nodes()): if people % 1000 == 0: user_mutiple_1000.append(people) # sort in ascending order of userID user_mutiple_1000.sort() # using number of frined function for friend in user_mutiple_1000: friend_list = recommend_by_number_of_common_friends(facebook, friend) # get the first 10 result by using list slicing print(friend, '(by number_of_common_friends):', friend_list[:10]) ### # Problem 7 ### print() print(&quot;Problem 7:&quot;) print() # using influence score function in the user listed that we created above for people in user_mutiple_1000: influence_recommend = recommend_by_influence(facebook, people) # get the first 10 result by using list slicing print(people, '(by influence):', influence_recommend[:10]) ### # Problem 8 ### print() print(&quot;Problem 8:&quot;) print() same_number = 0 diff_number = 0 for people in user_mutiple_1000: influence_method = recommend_by_influence(facebook, people) common_method = recommend_by_number_of_common_friends(facebook, people) if influence_method == common_method: same_number += 1 else: diff_number += 1 print('Same:', same_number) print('Different:', diff_number) if __name__ == &quot;__main__&quot;: main() ","link":"https://irisdin.github.io/post/ZT9SQzev5/"},{"title":"# 皮囊，魂灵；是石头还是剪刀","content":"猜拳永远是不可控的未知数，石头永远是剪刀的赢家，皮囊和魂灵中，哪一个可以生存 大众审美 在当今时代，姣好的容貌成了某种话语权，⽽⼤众的审美标准也似乎被禁锢了并呈⼀直下降的趋势。 以某短视频平台为例，⽤⼾总是乐此不疲的⽤⼀样的模板，沉迷于流⽔线式的审美。在资本的推波助 澜下，“⽩，幼，瘦” 的主流审美逐渐形成，审美多元化也在社会中慢慢被吞噬。在居伊.德波的《景观社会》中提到：“在⽣活中，如果⼈们完全顺从于景观的统治，逐步远离⼀切可能的切⾝体验，并 由此越来越难以找到个⼈的喜好，那么，这种状态就⽆可避免地就会造成对个性的抹杀”。这句话⽆ 疑是对当今⼤众审美同化的印证。所谓的⼤众审美和被灌输的畸形的审美，究竟困住了多少⼈，让多 少⼈陷⼊了所谓的容貌焦虑当中？ 我们需要“审美多元化”/浅谈不同⼈对于美的⻅解 美没有标准答案，美不应该被定义，⽩幼瘦也不应该成为美丽的标签 美不仅仅仅限于”⽩，幼，瘦“，它应该是多元化的。西⽅对于审美似乎更加多元化，丹凤眼扁平脸 可以是美的，⼩⻨⾊健康肤⾊也可以是美的，没有肌⾁的男⽣也是富有魅⼒的。美丽是主观的，每个 ⼈都有着不⼀样的答案。参与圆桌的各位探讨了各⾃认为的美，有⼈说：“相处起来和谐，舒服的⼈ 就是美丽的，我并不认同当今所推崇的流⽔线式的美丽”。也有⼈说了他所认为的美的特质：较⾼的 ⾝⾼，挺拔的⿐⼦...同样的在娱乐圈⾥，有些⼈喜欢以硬汉形式出现的有棱⻆的男性美，也有⼈喜欢 柔美精致的美丽。芭⽐，作为世界上对于孩⼦最具有影响⼒的玩偶，传递了⼀个健康，多元化的审 美。芭⽐没有被塑造成完美的形象，⽽橱窗⾥摆放着是不同肤⾊，⾝⾼，⾝材的各式各样的娃娃。 所以，活在审美⾥是漂亮，活出⾃⼰才是真正的美。 现在是⼀个看脸的时代吗 被外表所吸引是⽣物本能。在⾃然界，动物的外表⾼⼤对称、强壮健康，通常意味着拥有良好基因， 会获得更多的繁衍机会。到了⼈类社会⾥，看谁的脸，看什么样的脸，则是性别权利的表现。 颜值即正义这样的句⼦也有在社会中真实的例⼦。有⼀个叫做市桥达也的⽇本罪犯在07年的时候⽤极 其残忍的⼿法奸杀了⽆辜的英国⼥孩林赛·安妮·霍克并折磨了安妮⼗⼏个⼩时。在现实中，这样的杀 ⼈犯就应该遭受社会的唾骂，处以最严厉的酷刑。但是他的脸改变了⼤家对他的看法，在他的照⽚被 媒体公布出来后，很多⼥性迷上了他漫画⼀般的脸庞。更为荒唐的是，这些⼥性为这名⼗恶不赦的杀 ⼈犯成⽴了后援会，并要求判他⽆罪。在新闻学上有个词，叫“舆论失焦”，通俗的说是说事件经过 ⽹络发展，公众讨论后使舆论被⼀⽅主导，导致原本的主题向其他的⽅向发展。⼈们把关注点从命案 转移到了”脸“上，这反映了看脸的世界，但也同样是可悲的。 ⽪囊给⼈带来视觉审美，若这副⽪囊沾满⾎腥，他还是美的吗？ 所以我们是不是只要⼀味的追求美丽的⽪囊就能改天换命呢？娱乐新闻⾥，依靠颜值吸引富⼈，通过婚姻改变命运的故事，我们当然看过很多，但真实情况要⽐单纯看脸复杂：我们且不说再美丽的外表 也会有审美疲劳的⼀天。会玩⾦钱游戏的富⼈，当然都明⽩这个道理。所以，在花边新闻⾥，他们会频繁地更换所谓⼥朋友，但能让他们认为值得以婚姻形式来买单的对象，就绝不仅仅是看脸这么简 单。⾯对⼀个会不断贬值⽽且会引起审美疲劳的东西，稍微具备财务知识的⼈就懂得，最好的策略 是“只租⽽不买”。在⼯作中也是⼀样，公司的⻓期发展离不开有能⼒的员⼯带领团队。业绩和能⼒ 可以决定⼀个公司的利润和⻓期发展，但是靠单纯的美貌来运营商业的理论是不存在的，也是不合理 的。我们得出的结论是，这是个⼀开始看脸，但处久看脑的世界。⽆可厚⾮好看的⽪囊就像充值了会员⼀ 样，拥有更多的权⼒和优势。但是这不是⼀个”只“看脸的时代。“看脸”只是⼀个开始。要维持已 经取得的优势，必然得有⼀些其他的价值和资源来丰富⾃⼰，⾄少是对冲颜值的下滑。从另⼀个⽅向 看，为什么有⼀些初看貌不惊艳的⼈，会越来越有魅⼒，甚⾄你开始觉得他⻓得也很美了，也是这个 原因。有“有趣灵魂”进驻之后，好看的⽪囊就不会千篇⼀律。灵魂乏味，只会是“画⽪”。 credit to editor: yunyi ding /zizhan li/ chen zhang/ xinzhe he team levervision ","link":"https://irisdin.github.io/post/54PDWzCQ0/"},{"title":"Fraud detection pratice","content":"readme link🔻 uw cse only for practice purpose/plagiarism is not allowed🔻 cse misconduct guideline # importing the packages we used for this assignment import utils # noqa: F401, do not remove if using a Mac import matplotlib.pyplot as plt import random import csv def extract_election_vote_counts(filename, column_names): &quot;&quot;&quot; Returns a list of integers that contains the values in those columns from every row (the order of the integers does not matter). examples: extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Ahmadinejad&quot;, &quot;Rezai&quot;, &quot;Karrubi&quot;, &quot;Mousavi&quot;]) Should return: [1131111, 16920, 7246, 837858, 623946, 12199, 21609, 656508, ... Arguments: file_name: the election csv file column_name: list of column names for each province Returns: a list of integers that contains the values in those columns *update for problem 8: When data is missing, our calculation should ignore that data. Do not transform it into a zero. &quot;&quot;&quot; # creating an new empty list cleaned_data = [] # open the file election_file = open(filename) # reading lines from a csv file for line in csv.DictReader(election_file): for characters in column_names: # ignore the empty space in the csv file if len(line[characters]) &gt; 0: # replace unwanted characters with the empty string number = int(line[characters].replace(&quot;,&quot;, &quot;&quot;)) cleaned_data.append(int(number)) election_file.close() # Returns a list of integers that contains the values we want return cleaned_data def ones_and_tens_digit_histogram(numbers): &quot;&quot;&quot; taking as inputs a list of numbers and produce as ouput a list of 10 numbers. examples: the call ones_and_tens_digit_histogram([127, 426, 28, 9, 90]) should return [0.2, 0.0, 0.3, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.2] Arguments: numbers: a list of numbers Returns: In the returned list, the value at index i is the frequency with which digit i appeared in the ones place OR the tens place in the input list. In the input, in a number that is less than 10, such as 3, the tens place is implicitly zero. That is, 3 must be treated as 03. &quot;&quot;&quot; # creating new empty list ten_list = [] convert_num_list = [] # converting the number into two digits for n in numbers: n = n % 100 convert_num_list.append(n) for i in range(0, 10): frequency = 0 # finding the tens place and ones place for num in convert_num_list: if int(num // 10) == i: frequency += 1 if int(num % 10) == i: frequency += 1 # finding the total length of digit for calculation digits_length = len(numbers) * 2 # returning the list based by the calculation ten_list.append(frequency/(digits_length)) return ten_list def plot_iran_least_digits_histogram(histogram): &quot;&quot;&quot;&quot;&quot; Takes a histogram (as created by ones_and_tens_digit_histogram) Graphs the frequencies of the ones and tens digits for the Iranian election data. Save your plot to a file named iran-digits.png using plt.savefig. Arguments: histogram: the list of numbers with the frequencies of the ones and tens digits Return: The function should not return anything but we need to show and save the plot &quot;&quot;&quot; # creating the plot based on the example x_axis = list(range(0, 10)) plt.title('Distribution of last two digits in Iranian dataset') plt.xlabel(&quot;Digit&quot;) plt.ylabel(&quot;Frequency&quot;) # The ideal line represents the uniform distribution ideal_line = [0.1] * 10 # creating the graph with the detail that given plt.plot(x_axis, ideal_line, color='blue', label='ideal') plt.plot(x_axis, histogram, color='orange', label='iran') plt.legend(loc='upper left') plt.savefig(&quot;iran-digits.png&quot;) plt.show() plt.clf() return None def plot_distribution_by_sample_size(): &quot;&quot;&quot; Creates five different collections (one of size 10, another of size 50, then 100, 1000, and 10,000) of random numbers where every element in the collection is a different random number x such that 0 &lt;= x &lt; 100. Argument: / Return: The function should not return anything but we need to show and save the graph. &quot;&quot;&quot; # graph the ideal line(* based on the format given) ideal_line = [0.1] * 10 # creating differnt collections of random numbers plt.plot(ideal_line, color='blue', label='ideal') for i in [10, 50, 100, 1000, 10000]: # the random_list function is a helper function that generate # random data graph = ones_and_tens_digit_histogram(random_list(i)) plt.plot(graph, label=str(i) + 'random numbers') # creating other details based on the example given plt.title('Distribution of last two digits in randomly generated samples') plt.xlabel(&quot;Digit&quot;) plt.ylabel(&quot;Frequency&quot;) plt.legend(loc='upper left') plt.savefig(&quot;random-digits.png&quot;) plt.show() plt.clf() def mean_squared_error(numbers1, numbers2): &quot;&quot;&quot; finding way to computationally determine how similar two lines are. determine whether the difference between lines A and B is larger or smaller than the difference between lines C and D. Argument: two lists of numbers Return: the mean squared error between the lists. &quot;&quot;&quot; mean_squared_error = 0.0 # finding the mse based on the definition for i in range(len(numbers1)): mean_squared_error += (numbers1[i] - numbers2[i])**2 mean_squared_error = mean_squared_error / (float)(len(numbers1)) return mean_squared_error def calculate_mse_with_uniform(histogram): &quot;&quot;&quot; Argument: histogram(as created by ones_and_tens_digit_histogram) Return: Returns the mean squared error of the given histogram with the uniform distribution. E.G.: call: calculate_mse_with_uniform(histogram) Should return: 0.000739583333333 &quot;&quot;&quot; # calling the mean square error function to return the # number we want uniform__his_mse = mean_squared_error(histogram, [0.1] * 10) return uniform__his_mse def random_list(number): &quot;&quot;&quot;&quot; This is a helper function to avoid duplicate code. The function randomly generated number that 0 &lt;= number &lt; 100 * this helper function can be used in the function plot_distribution_by_sample_size and using to compraing the election MSE with the sample Return: a list of random numbers that 0&lt;= number &lt; 100 &quot;&quot;&quot; # creating a new empty list random_lst = [] # generate random number x with the given range for x in range(number): random_lst.append(random.randint(0, 99)) return random_lst def Mse_election_to_samples(mse_election, election_datapoints, specific_election): &quot;&quot;&quot; this is a helper function to avoid dupicate codes when we try to implement different dataset(E.G: different countries election dataset) Determine how many of the 10,000 random MSEs are larger than or equal to MSE for election Determine how many of the 10,000 random MSEs are smaller than the MSE for election Determine the election null hypothesis rejection level Print all of these values Arguments:mse_election : the mean sqaured error of each specific eletion election_datapoints: number of datapoints(*we can not haedcode for that) specific_election: the title of file we used for the print statement return: this function should not return anything &quot;&quot;&quot; small_quan = 0 large_quan = 0 # we need to generate 10000 random MSEs for i in range(0, 10000): # calling the helper function to create random list random = random_list(election_datapoints) # finding the frequency his = ones_and_tens_digit_histogram(random) # finding the mean squared error mse = calculate_mse_with_uniform(his) # comparing of the sample mse with the real election mse if (mse &gt;= mse_election): large_quan += 1 else: small_quan += 1 # calculating the p value based on the definition # using the p value to compare p_value = large_quan / 10000 # the print statment we used for differnt elections # writing based on the format that given print(specific_election, &quot;election MSE:&quot;, mse_election) print(&quot;Quantity of MSEs larger than or equal to the&quot;, specific_election, &quot;election MSE:&quot;, large_quan) print(&quot;Quantity of MSEs smaller than the&quot;, specific_election, &quot;election MSE:&quot;, small_quan) print(specific_election, &quot;election null hypothesis rejection level p:&quot;, p_value) return None def compare_iran_mse_to_samples(iran_mse, number_of_iran_datapoints): &quot;&quot;&quot; Determine how many of the 10,000 random MSEs are larger than or equal to the Iran MSE Determine how many of the 10,000 random MSEs are smaller than the Iran MSE Determine the Iranian election null hypothesis rejection level Print all of these values after the Iranian MSE. Arguments: the Iranian MSE (as computed by calculate_mse_with_uniform) the number of data points in the Iranian dataset Return: This function should not return anything. &quot;&quot;&quot; # calling the helper function above to generate iran result MSE_election_to_samples(iran_mse, number_of_iran_datapoints, &quot;2009 Iranian&quot;) return None def compare_us_mse_to_samples(us_mse, number_of_us_datapoints): &quot;&quot;&quot; works the same as the compare_iran_mse_to_samples function * just replacing the dataset to real us election &quot;&quot;&quot; # calling the helper function above to generate iran result MSE_election_to_samples(us_mse, number_of_us_datapoints, &quot;2008 United States&quot;) return None # The code in this function is executed when this # file is run as a Python program # calling the function that written above def main(): iran_election = ['Ahmadinejad', 'Rezai', 'Karrubi', 'Mousavi'] vote_count_iran = extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, iran_election) iran_his = ones_and_tens_digit_histogram(vote_count_iran) iran_error = calculate_mse_with_uniform(iran_his) plot_iran_least_digits_histogram(iran_his) plot_distribution_by_sample_size() # we do not need to hard code the number of datapoints compare_iran_mse_to_samples(iran_error, len(vote_count_iran)) # formating requirement print() # * using other dataset(the code works the same way as above) us_election = [&quot;Obama&quot;, &quot;McCain&quot;, &quot;Nader&quot;, &quot;Barr&quot;, &quot;Baldwin&quot;, &quot;McKinney&quot;] vote_count_us = extract_election_vote_counts(&quot;election-us-2008.csv&quot;, us_election) us_his = ones_and_tens_digit_histogram(vote_count_us) us_error = calculate_mse_with_uniform(us_his) compare_us_mse_to_samples(us_error, len(vote_count_us)) if __name__ == &quot;__main__&quot;: main() testing import fraud_detection as fd import math def test_extract_election_vote_counts(): actual = fd.extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Ahmadinejad&quot;, &quot;Rezai&quot;, &quot;Karrubi&quot;, &quot;Mousavi&quot;])[:8] expected = [1131111, 16920, 7246, 837858, 623946, 12199, 21609, 656508] assert actual == expected actual = fd.extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Ahmadinejad&quot;])[:10] expected = [1131111, 623946, 325911, 1799255, 199654, 299357, 3819495, 359578, 285984, 2214801] assert actual == expected actual = fd.extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Karrubi&quot;])[:10] expected = [7246, 21609, 2319, 14579, 7471, 3563, 67334, 4127, 928, 13561] assert actual == expected actual = actual = fd.extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Mousavi&quot;])[:10] expected = [837858, 656508, 302825, 746697, 96826, 177268, 3371523, 106099, 90363, 884570] assert actual == expected print(&quot;test_extract_election_vote_counts passed.&quot;) def test_ones_and_tens_digit_histogram(): actual = fd.ones_and_tens_digit_histogram([127, 426, 28, 9, 90]) expected = [0.2, 0.0, 0.3, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.2] actual = fd.ones_and_tens_digit_histogram([0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765]) expected = [0.21428571428571427, 0.14285714285714285, 0.047619047619047616, 0.11904761904761904, 0.09523809523809523, 0.09523809523809523, 0.023809523809523808, 0.09523809523809523, 0.11904761904761904, 0.047619047619047616] actual = fd.ones_and_tens_digit_histogram([3, 5, 15, 16, 18]) expected = [0.2, 0.3, 0.0, 0.2, 0.0, 0.1, 0.1, 0.0, 0.1, 0.0] actual = fd.ones_and_tens_digit_histogram([123, 88, 1314, 279, 236556]) expected = [0.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.1] actual = fd.ones_and_tens_digit_histogram([10, 10]) expected = [0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] for i in range(len(actual)): assert math.isclose(actual[i], expected[i]) print(&quot;test_ones_and_tens_digit_histogram passed.&quot;) def test_mean_squared_error(): assert fd.mean_squared_error([1, 4, 9], [6, 5, 4]) == 17.0 assert fd.mean_squared_error([23, 37], [33, 49]) == 122.0 assert fd.mean_squared_error([123, 456, 789], [133, 466, 799]) == 100.0 print(&quot;test_mean_squared_error passed.&quot;) def test_calculate_mse_with_uniform(): dataset = fd.extract_election_vote_counts(&quot;election-iran-2009.csv&quot;, [&quot;Ahmadinejad&quot;, &quot;Rezai&quot;, &quot;Karrubi&quot;, &quot;Mousavi&quot;]) frequency = fd.ones_and_tens_digit_histogram(dataset) actual = fd.calculate_mse_with_uniform(frequency) expected = 0.000739583333333 assert math.isclose(actual, expected) print(&quot;test_calculate_mse_with_uniform passed.&quot;) def main(): test_extract_election_vote_counts() test_ones_and_tens_digit_histogram() test_mean_squared_error() test_calculate_mse_with_uniform() print(&quot;All test passed.&quot;) if __name__ == &quot;__main__&quot;: main() ","link":"https://irisdin.github.io/post/HsYyfFoIA/"},{"title":"K-Means algorithms practice","content":"readme link🔻 uw cse only for practice purpose/plagiarism is not allowed🔻 [cse misconduct guideline](https://www.cs.washington.edu/academics/misconduct import matplotlib.pyplot as plt # noqa: E402 import os # noqa: E402 import math # noqa: E402 from utils import converged, plot_2d, plot_centroids, assert_equals, \\ read_data, load_centroids, write_centroids_tofile # noqa: E402 # problem for students def euclidean_distance(point1, point2): &quot;&quot;&quot;Calculate the Euclidean distance between two data points. Arguments: point1: a non-empty list of floats representing a data point point2: a non-empty list of floats representing a data point Returns: the Euclidean distance between two data points Example: Code: point1 = [1.1, 1, 1, 0.5] point2 = [4, 3.14, 2, 1] print(euclidean_distance(point1, point2)) Output: 3.7735394525564456 &quot;&quot;&quot; point_distance = 0.0 for i in range(len(point1)): # using the formula to find the distance between two points point_distance += (point1[i] - point2[i]) ** 2 return math.sqrt(point_distance) # problem for students def get_closest_centroid(point, centroids_dict): &quot;&quot;&quot;Given a datapoint, finds the closest centroid. You should use the euclidean_distance function (that you previously implemented). Arguments: point: a list of floats representing a data point centroids_dict: a dictionary representing the centroids where the keys are strings (centroid names) and the values are lists of centroid locations Returns: a string as the key name of the closest centroid to the data point Example: Code: point = [0, 0, 0, 0] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [2, 2, 2, 2]} print(get_closest_centroid(point, centroids_dict)) Output: centroid1 &quot;&quot;&quot; max = 100000000000000000000000000000 close = '' for i in centroids_dict.keys(): d = euclidean_distance(point, centroids_dict[i]) # Comparing until we find the cloest distance if d &lt; max: max = d close = i return close # problem for students def update_assignment(list_of_points, centroids_dict): &quot;&quot;&quot;Assign all data points to the closest centroids. You should use the get_closest_centroid function (that you previously implemented). Arguments: list_of_points: a list of lists representing all data points centroids_dict: a dictionary representing the centroids where the keys are strings (centroid names) and the values are lists of centroid locations Returns: a new dictionary whose keys are the centroids' key names and values are lists of points that belong to the centroid. If a given centroid does not have any data points closest to it, do not include the centroid in the returned dictionary Example: Code: list_of_points = [[1.1, 1, 1, 0.5], [4, 3.14, 2, 1], [0, 0, 0, 0]] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [2, 2, 2, 2]} print(update_assignment(list_of_points, centroids_dict)) Output: {'centroid1': [[1.1, 1, 1, 0.5], [0, 0, 0, 0]], 'centroid2': [[4, 3.14, 2, 1]]} &quot;&quot;&quot; # creating a new dictionary dic = {} ls = list(centroids_dict.keys()) for i in ls: dic[i] = [] for j in list_of_points: s = get_closest_centroid(j, centroids_dict) dic[s].append(j) dic1 = {} for i in dic.keys(): if len(dic[i]) != 0: dic1[i] = dic[i] return dic1 # problem for students def mean_of_points(list_of_points): &quot;&quot;&quot;Calculate the mean of a given group of data points. You should NOT hard-code the dimensionality of the data points). Arguments: list_of_points: a list of lists representing a group of data points Returns: a list of floats as the mean of the given data points Example: Code: list_of_points = [[1.1, 1, 1, 0.5], [4, 3.14, 2, 1], [0, 0, 0, 0]] print(mean_of_points(list_of_points)) Output: [1.7, 1.3800000000000001, 1.0, 0.5] &quot;&quot;&quot; mean = [] for x in range(len(list_of_points[0])): sum = 0 for y in range(len(list_of_points)): sum += list_of_points[y][x] answer = sum / len(list_of_points) mean.append(answer) return(mean) # problem for students def update_centroids(assignment_dict): &quot;&quot;&quot;Update centroid locations as the mean of all data points that belong to the cluster. You should use the mean_of_points function (that you previously implemented). Arguments: assignment_dict: a dictionary whose keys are the centroids' key names and values are lists of points that belong to the centroid. It is the dictionary returned by update_assignment function. Returns: A new dictionary representing the updated centroids. If a given centroid does not have any data points closest to it, do not include the centroid in the returned dictionary. Example: Code: assignment_dict = {'centroid1': [[1.1, 1, 1, 0.5], [0, 0, 0, 0]], 'centroid2': [[4, 3.14, 2, 1]]} print(update_centroids(assignment_dict)) Output: {'centroid1': [0.55, 0.5, 0.5, 0.25], 'centroid2': [4.0, 3.14, 2.0, 1.0]} &quot;&quot;&quot; new_version = {} for a, b in assignment_dict.items(): new_version[a] = mean_of_points(b) return new_version # ---------------------------------------------------------- # HELPER FUNCTIONS def setup_data_centroids(): &quot;&quot;&quot;Creates are returns data for testing k-means methods. Returns: list_of_points, a list of data points centroids_dict1, two 4D centroids centroids_dict2, two 4D centroids &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### list_of_points = [ [-1.01714716, 0.95954521, 1.20493919, 0.34804443], [-1.36639346, -0.38664658, -1.02232584, -1.05902604], [1.13659605, -2.47109085, -0.83996912, -0.24579457], [-1.48090019, -1.47491857, -0.6221167, 1.79055006], [-0.31237952, 0.73762417, 0.39042814, -1.1308523], [-0.83095884, -1.73002213, -0.01361636, -0.32652741], [-0.78645408, 1.98342914, 0.31944446, -0.41656898], [-1.06190687, 0.34481172, -0.70359847, -0.27828666], [-2.01157677, 2.93965872, 0.32334723, -0.1659333], [-0.56669023, -0.06943413, 1.46053764, 0.01723844] ] centroids_dict1 = { &quot;centroid1&quot;: [0.1839742, -0.45809263, -1.91311585, -1.48341843], &quot;centroid2&quot;: [-0.71767545, 1.2309971, -1.00348728, -0.38204247], } centroids_dict2 = { &quot;centroid1&quot;: [0.1839742, -0.45809263, -1.91311585, -1.48341843], &quot;centroid2&quot;: [10, 10, 10, 10], } return list_of_points, centroids_dict1, centroids_dict2 # ---------------------------------------------------------- # TESTS def test_euclidean_distance(): &quot;&quot;&quot;Function for verifying if euclidean_distance is correctly implemented. Will throw an error if it isn't. &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### # simple case point1 = [0, 0, 0, 0] point2 = [1, 1, 1, 1] assert_equals(2, euclidean_distance(point1, point2)) # negative point1 = [-1, -1, -1, -1] point2 = [-5, -3, -1, -1] assert_equals(math.sqrt(20), euclidean_distance(point1, point2)) # floats point1 = [1.1, 1, 1, 0.5] point2 = [4, 3.14, 2, 1] assert_equals(math.sqrt(14.2396), euclidean_distance(point1, point2)) # long version point1 = [-0.451, 0.535, 1.031, 1.097, 0.59, -0.435, 1.934, 0.227, 1.026, 0.427, 0.267, -1.482, -0.636, 0.354, -0.675, -0.751, -0.719, -0.454, -1.262, -0.326, -0.608, -0.22, 0.354, 1.048, -0.92, -0.027, 0.328, 1.397, 0.05, -0.125, 0.329, 1.631, -1.127, 0.067, 0.755, 1.367, 0.162, -0.072, 0.289, 2.388, 1.127, -0.706, 1.186, 0.815, -0.305, -1.001, -0.389, -0.871, 0.794, 0.5, 0.741, -0.348, -0.29, -0.924, 0.241, 0.16, -0.315, -0.149, -0.457, 0.616, -0.017, 0.386, 1.34, -0.311, -1.116, -1.706, -1.517, 0.781, 0.514, 1.126, -0.665, 0.583, -0.07, -0.192, -0.083, -0.624, 0.582, 0.502, 0.98, -0.39, 0.438, -0.023, -1.097, -1.149, 0.666, -0.831, -0.048, -1.257, 1.043, -1.676, -0.752, 1.964, -1.332, 0.057, -0.061, -0.858, -0.817, 0.92, -0.041, -0.364] point2 = [-0.625, -0.902, -0.869, 0.348, -1.461, 1.61, 0.34, 0.187, 0.232, -0.802, -0.666, 0.168, 0.898, 0.854, 1.668, -1.964, 0.745, -0.512, 0.034, 0.523, -1.01, -0.691, 1.542, 0.174, 1.026, 0.636, -0.185, -0.582, -3.384, 0.876, -0.418, 1.623, -0.224, 0.869, 1.38, -0.45, 0.021, 1.766, 0.915, -1.002, 1.464, 0.361, 0.407, -0.312, -0.623, 1.203, -0.776, 2.283, 0.73, 0.151, 0.393, -0.852, 1.286, 0.171, 0.306, 0.675, -0.283, -0.367, -0.556, -1.865, 1.194, 0.605, 1.309, -0.594, -0.715, -0.88, 1.115, -0.625, -1.915, -0.853, 0.489, -1.729, 1.105, -0.822, 0.13, 0.986, -0.459, 2.506, 0.997, 1.511, 0.412, 0.034, 0.109, 0.068, -0.267, -0.034, 1.614, -0.939, -0.06, -0.112, 0.026, -0.526, 0.608, 0.845, 0.424, 0.693, 0.209, -1.142, -0.666, 0.47] expected = 13.52044903100485 received = euclidean_distance(point1, point2) assert_equals(expected, received) print(&quot;test_euclidean_distance passed.&quot;) def test_get_closest_centroid(): &quot;&quot;&quot;Function for verifying if get_closest_centroid is correctly implemented. Will throw an error if it isn't. &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### # set up point1 = [0, 0, 0, 0] point2 = [1.1, 5.3, 55, -12.1] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [-10.1, 1, 23.2, 5.099]} assert_equals(&quot;centroid1&quot;, get_closest_centroid(point1, centroids_dict)) assert_equals(&quot;centroid2&quot;, get_closest_centroid(point2, centroids_dict)) point3 = [10.1, 1, 23.2, 5.099] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [10, 1, 23, 5], &quot;centroid3&quot;: [-100, 20.2, 52.9, -37.088]} assert_equals(&quot;centroid2&quot;, get_closest_centroid(point3, centroids_dict)) print(&quot;test_get_closest_centroid passed.&quot;) def test_update_assignment(): &quot;&quot;&quot;Function for verifying if update_assignment is correctly implemented. Will throw an error if it isn't. &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### # set up list_of_points, centroids_dict1, centroids_dict2 = setup_data_centroids() # centroids_dict1 received = update_assignment(list_of_points, centroids_dict1) expected = { &quot;centroid1&quot;: [[-1.36639346, -0.38664658, -1.02232584, -1.05902604], [1.13659605, -2.47109085, -0.83996912, -0.24579457], [-0.83095884, -1.73002213, -0.01361636, -0.3265274]], &quot;centroid2&quot;: [[-1.01714716, 0.95954521, 1.20493919, 0.34804443], [-1.48090019, -1.47491857, -0.6221167, 1.79055006], [-0.31237952, 0.73762417, 0.39042814, -1.1308523], [-0.78645408, 1.98342914, 0.31944446, -0.41656898], [-1.06190687, 0.34481172, -0.70359847, -0.27828666], [-2.01157677, 2.93965872, 0.32334723, -0.1659333], [-0.56669023, -0.06943413, 1.46053764, 0.01723844]] } assert_equals(expected, received) # centroids_dict2 received = update_assignment(list_of_points, centroids_dict2) expected = { &quot;centroid1&quot;: [[-1.36639346, -0.38664658, -1.02232584, -1.05902604], [1.13659605, -2.47109085, -0.83996912, -0.24579457], [-0.83095884, -1.73002213, -0.01361636, -0.3265274], [-1.01714716, 0.95954521, 1.20493919, 0.34804443], [-1.48090019, -1.47491857, -0.6221167, 1.79055006], [-0.31237952, 0.73762417, 0.39042814, -1.1308523], [-0.78645408, 1.98342914, 0.31944446, -0.41656898], [-1.06190687, 0.34481172, -0.70359847, -0.27828666], [-2.01157677, 2.93965872, 0.32334723, -0.1659333], [-0.56669023, -0.06943413, 1.46053764, 0.01723844]] } assert_equals(expected, received) print(&quot;test_update_assignment passed.&quot;) def test_mean_of_points(): &quot;&quot;&quot;Function for verifying if mean_of_points is correctly implemented. Will throw an error if it isn't. &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### # super simple list_of_points = [ [0, 0, 0, 0], [0, 0, 0, 0], ] assert_equals([0, 0, 0, 0], mean_of_points(list_of_points)) # a little more complicated list_of_points = [ [1, 2, 4, 6], [3, 4, 6, 8], ] assert_equals([2, 3, 5, 7], mean_of_points(list_of_points)) # negative list_of_points = [ [-1, -10, -70, -89], [2, 3, 55, 7], ] assert_equals([0.5, -3.5, -7.5, -41], mean_of_points(list_of_points)) # long version list_of_points = [[0.339, -0.65, 0.596, 0.804], [0.002, 0.973, -0.194, 0.016], [-0.121, -1.241, -0.69, 0.74], [-0.742, -0.033, -0.322, -0.536], [-0.434, -0.775, 0.943, -1.224], [-2.72, 0.955, -0.072, 0.392], [0.148, -0.939, 1.471, 1.217], [-0.226, 0.42, -0.687, 1.799], [-1.156, -0.69, 1.287, -0.984], [-0.625, 0.555, -0.025, -0.391]] expected = [-0.5535, -0.14250000000000002, 0.2307, 0.18330000000000002] received = mean_of_points(list_of_points) assert_equals(expected, received) print(&quot;test_mean_of_points passed.&quot;) def test_update_centroids(): &quot;&quot;&quot;Function for verifying if update_centroids is correctly implemented. Will throw an error if it isn't. &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### # set up list_of_points, centroids_dict1, centroids_dict2 = setup_data_centroids() # centroids_dict1 assignment_dict = update_assignment(list_of_points, centroids_dict1) expected = { 'centroid2': [-1.03386497, 0.774388037, 0.33899735, 0.023455955], 'centroid1': [-0.35358541, -1.529253186, -0.62530377, -0.543782673] } received = update_centroids(assignment_dict) assert_equals(expected, received) # centroids_dict2 assignment_dict = update_assignment(list_of_points, centroids_dict2) expected = { 'centroid1': [-0.82978110, 0.08329567, 0.04970701, -0.146715632] } received = update_centroids(assignment_dict) assert_equals(expected, received) print(&quot;test_update_centroids passed.&quot;) # main functions def main_test(): ####################################################### # You do not need to change anything in this function # ####################################################### test_euclidean_distance() test_get_closest_centroid() test_update_assignment() test_mean_of_points() test_update_centroids() print(&quot;all tests passed.&quot;) def main_2d(data, init_centroids): ####################################################### # You do not need to change anything in this function # ####################################################### centroids = init_centroids old_centroids = None step = 0 while not converged(centroids, old_centroids): # save old centroid old_centroids = centroids # new assignment assignment_dict = update_assignment(data, old_centroids) # update centroids centroids = update_centroids(assignment_dict) # plot centroid fig = plot_2d(assignment_dict, centroids) plt.title(f&quot;step{step}&quot;) fig.savefig(os.path.join(&quot;results&quot;, &quot;2D&quot;, f&quot;step{step}.png&quot;)) plt.clf() step += 1 print(f&quot;K-means converged after {step} steps.&quot;) return centroids def main_mnist(data, init_centroids): ####################################################### # You do not need to change anything in this function # ####################################################### centroids = init_centroids # plot initial centroids plot_centroids(centroids, &quot;init&quot;) old_centroids = None step = 0 while not converged(centroids, old_centroids): # save old centroid old_centroids = centroids # new assignment assignment_dict = update_assignment(data, old_centroids) # update centroids centroids = update_centroids(assignment_dict) step += 1 print(f&quot;K-means converged after {step} steps.&quot;) # plot final centroids plot_centroids(centroids, &quot;final&quot;) return centroids if __name__ == &quot;__main__&quot;: # main_test() data, label = read_data(&quot;data/mnist.csv&quot;) init_c = load_centroids(&quot;data/mnist_init_centroids.csv&quot;) final_c = main_mnist(data, init_c) write_centroids_tofile(&quot;mnist_final_centroids.csv&quot;, final_c) analysis from kmeans import get_closest_centroid from utils import load_centroids, read_data, assert_equals # ---------------------------------------------------------- # PROBLEMS FOR STUDENTS def update_assignment(list_of_points, labels, centroids_dict): &quot;&quot;&quot;Assign all data points to the closest centroids and keep track of their labels. The i-th point in &quot;data&quot; corresponds to the i-th label in &quot;labels&quot;. Arguments: list_of_points: a list of lists representing all data points labels: a list of ints representing all data labels labels[i] is the label of the point list_of_points[i] centroids_dict: a dictionary representing the centroids where the keys are strings (centroid names) and the values are lists of centroid locations Returns: a new dictionary whose keys are the centroids' key names and values are a list of labels of the data points that are assigned to that centroid. Example: Code: list_of_points = [[1.1, 1, 1, 0.5], [4, 3.14, 2, 1], [0, 0, 0, 0]] labels = [2, 1, 3] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [2, 2, 2, 2]} print(update_assignment(list_of_points, labels, centroids_dict)) Output: {'centroid1': [2, 3], 'centroid2': [1]} &quot;&quot;&quot; # creating a new dictionary dic = {} for i in list_of_points: # get the return of the function as key key = get_closest_centroid(i, centroids_dict) # give every dic[key] an empty list dic[key] = [] for i in list_of_points: # get the value in the labels with the certain index value = labels[list_of_points.index(i)] # get the key key = get_closest_centroid(i, centroids_dict) # give the value to the certain key dic[key].append(value) return dic def majority_count(labels): &quot;&quot;&quot;Return the count of the majority labels in the label list Arguments: labels: a list of labels Returns: the count of the majority labels in the list Example: Code: labels = [0, 3, 3, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 2, 2, 2] print(majority_count(labels)) Output: 6 &quot;&quot;&quot; dic = {} for i in labels: # initial the dic dic[i] = 0 for i in labels: # give the value to the dic if i in the dic.keys() let the # value of dic add one dic[i] += 1 max = 0 for i in list(dic.values()): if i &gt; max: max = i # assign the i to max we can get the max return max def accuracy(list_of_points, labels, centroids_dict): &quot;&quot;&quot;Calculate the accuracy of the algorithm. You should use update_assignment and majority_count (that you previously implemented) Arguments: list_of_points: a list of lists representing all data points labels: a list of ints representing all data labels labels[i] is the label of the point list_of_points[i] centroids_dict: a dictionary representing the centroids where the keys are strings (centroid names) and the values are lists of centroid locations Returns: a float representing the accuracy of the algorithm Example: Code: list_of_points = [[1.1, 1, 1, 0.5], [4, 3.14, 2, 1], [0, 0, 0, 0]] labels = [2, 1, 3] centroids_dict = {&quot;centroid1&quot;: [1, 1, 1, 1], &quot;centroid2&quot;: [2, 2, 2, 2]} print(accuracy(list_of_points, labels, centroids_dict)) Output: 0.6666666666666666 &quot;&quot;&quot; sum = 0 v = update_assignment(list_of_points, labels, centroids_dict).values() for i in v: # get the sum of majority_count sum += majority_count(i) # get the value according to the definition return sum / len(labels) # ---------------------------------------------------------- # HELPER FUNCTIONS def setup_for_tests(): &quot;&quot;&quot;Creates are returns data for testing analysis methods. Returns: data, a list of data points labels, numeric labels for each data point centroids_dict1, three 4D centroids centroids_dict2, three non-random 4D centroids with poor starting values &quot;&quot;&quot; ####################################################### # You do not need to change anything in this function # ####################################################### list_of_points = [ [-1.01714716, 0.95954521, 1.20493919, 0.34804443], [-1.36639346, -0.38664658, -1.02232584, -1.05902604], [1.13659605, -2.47109085, -0.83996912, -0.24579457], [-1.48090019, -1.47491857, -0.6221167, 1.79055006], [-0.31237952, 0.73762417, 0.39042814, -1.1308523], [-0.83095884, -1.73002213, -0.01361636, -0.32652741], [-0.78645408, 1.98342914, 0.31944446, -0.41656898], [-1.06190687, 0.34481172, -0.70359847, -0.27828666], [-2.01157677, 2.93965872, 0.32334723, -0.1659333], [-0.56669023, -0.06943413, 1.46053764, 0.01723844] ] labels = [0, 1, 0, 2, 1, 2, 1, 2, 0, 0] centroids_dict1 = { &quot;centroid1&quot;: [0.1839742, -0.45809263, -1.91311585, -1.48341843], &quot;centroid2&quot;: [-0.71767545, 1.2309971, -1.00348728, -0.38204247], &quot;centroid3&quot;: [-1.71767545, 0.29971, 0.00328728, -0.38204247], } centroids_dict2 = { &quot;centroid1&quot;: [0.1839742, -0.45809263, -1.91311585, -1.48341843], &quot;centroid2&quot;: [10, 10, 10, 10], &quot;centroid3&quot;: [-10, 1, -10, 10], } return list_of_points, labels, centroids_dict1, centroids_dict2 # ---------------------------------------------------------- # TESTS def test_update_assignment(): ####################################################### # You do not need to change anything in this function # ####################################################### # set up (list_of_points, labels, centroids_dict1, centroids_dict2) = setup_for_tests() # test with centroids_dict1 answer = {'centroid3': [0, 1, 2, 1, 2, 2, 0], 'centroid1': [0], 'centroid2': [1, 0]} assert_equals( update_assignment(list_of_points, labels, centroids_dict1), answer) # test with centroids_dict2 answer = {'centroid1': [0, 1, 0, 2, 1, 2, 1, 2, 0, 0]} assert_equals( update_assignment(list_of_points, labels, centroids_dict2), answer) print(&quot;test_update_assignment passed&quot;) def test_majority_count(): ####################################################### # You do not need to change anything in this function # ####################################################### # single assert_equals(6, majority_count([0, 0, 0, 0, 0, 0])) assert_equals(5, majority_count([1, 0, 0, 0, 0, 0])) assert_equals(5, majority_count([0, 1, 1, 1, 1, 1])) # mixed assert_equals(4, majority_count([0, 0, 1, 1, 0, 0])) assert_equals(4, majority_count([0, 2, 2, 2, 3, 3, 0, 1, 1, 0, 0])) # tied max count assert_equals(4, majority_count([0, 2, 2, 2, 0, 2, 0, 0])) print(&quot;test_majority_count passed&quot;) def test_accuracy(): ####################################################### # You do not need to change anything in this function # ####################################################### # set up (list_of_points, labels, centroids_dict1, centroids_dict2) = setup_for_tests() # test with centroids_dict1 expected = 0.5 received = accuracy(list_of_points, labels, centroids_dict1) assert_equals(expected, received) # test with centroids_dict2 expected = 0.4 received = accuracy(list_of_points, labels, centroids_dict2) assert_equals(expected, received) print(&quot;test_accuracy passed&quot;) def main_test(): ####################################################### # You do not need to change anything in this function # ####################################################### test_update_assignment() test_majority_count() test_accuracy() print(&quot;all tests passed.&quot;) if __name__ == &quot;__main__&quot;: centroids = load_centroids(&quot;mnist_final_centroids.csv&quot;) # Consider exploring the centroids data here # Uncomment the line below for Part 2 Step 2, 3, and 4: # main_test() data, label = read_data(&quot;data/mnist.csv&quot;) print(accuracy(data, label, centroids)) ","link":"https://irisdin.github.io/post/SEbRoAqB2/"},{"title":"Should employers be allowed to surveil their employees' or potential employee’s digital actions and made decisions?","content":" Social media provided a platform which people can share and post anything related to their daily lives. In 2020, there are over 3.6 billion people who are using social media (Tankovska, 2021) and the number are still increasing with the development of our society. At the same time, more and more employers and human resources mangers start to use the social media to trace the candidates and their employees. According to a survey, 60% percent of the employers uses social media to trace their potential candidates and employees. (“Number of Employers Using Social Media to Screen Candidates Has Increased 500 Percent over the Last Decade”,2016) Also, many employees are fired because of certain post they spread on the social media. However, should the employers have the right to monitor their employees' or potential employee’s social media and make decisions? The three primary groups who concerned about this problem are the current/potential employees and employer. As many employers are tracing the social media and supporting the usage of surveillance on social media, many employees argued that this behavior invaded their privacy and also leads to some personal bias that unrelated to the working abilities. From my perspective, it is necessary and important to trace their digital actions for several reasons: allowing employer to trace the social media helps to protect the profits of the company and social media serves a great role in selecting the candidates. To begin with, monitoring employee’s social media helps to protect the overall benefits of the company. In the workplace, majority of workers are being affected by heavy work tasks, high intensity working environment, and increasing work pressure. Under that circumstances, many workers choose to express their emotions and dissatisfaction with work on social platforms. These posts and some mis-behavior on the Internet will influence the company ‘s reputation directly. Moreover, lots of employees post something that should not be posted online. Contents such as product information, proprietary information, company information or other sensitive data may be unintentionally or intentionally leaked through social media channels. Sometimes, these post on the social media will lead to some legal actions which cost lots of time and effort of a company. Here is an example. In the case of Tianmai Juyan Education Co., Ltd. v. Ma, which is a labor dispute, (“Tianmai Juyuan (Beijing) Education Technology Co., Ltd. and Ma Wei’s first-instance civil judgment”, 2018) Ma was angry because of the company’s action of salary reduction. Thus, he posted a screenshot of the company’s private incentive plan and the company’s internal salary reduction notice in a Chinese social media called WeChat. The Second Intermediate People's Court of Beijing proposes that after Ma posted information related to the company's internal affairs to WeChat, people around him saw the post and left some comments within the social media. As a result, his behavior caused the dissemination and leakage of the company's internal business information. Regardless of whether the information posted is a trade secret of the company, Ma, as an employee, should not arbitrarily post company internal information to social media due to his personal unsatisfaction. His behavior obviously violates the employee's duty of loyalty and is an excessive expression of wage reduction. The social media contains huge quantity of people, and one post could be spread soon to tremendous amount of people and ruined a company. The monitoring of employee’s social media like Facebook or Instagram can help the company to find and solve some potential problems quicky. Also, according to the Federal Rules of Civil Procedure (FRCP), all work-related content posted by employees on social media must be visible, regardless of whether the content is created through a personal account. Therefore, the company could reach out and deal some inappropriate post as soon as possible. Another aspect is that social media is a great tool for the human resources manager to select the candidates besides the resume they provided. From the resume, the employer can see your abilities and official grades while the personal profile just indicated your personality. Through the research of more than five hundred college students and their Facebook homepages, (knowledgeatwharton,2012) the researchers found the college students’ &quot;Facebook&quot; personal homepage displayed the so-called &quot;Five Personality Traits&quot;:Easy-going, Consciousness, Extroversion, Emotional Stability and Openness. After communicating with the employers of some of these students, the researchers found out that the students' working performance is closely related to the two personality traits: emotional stability and easygoing. Combing the resume and social media can help to draw a real personal profile of a potential candidate. And the top three social media that the most employers are likely to check are: LinkedIn, Facebook, and Twitter. (“Social media Screenings gain in popularity, 2020”) Besides personality, it is also important to check the candidates’ online persona to make sure they provided a positive image. For example, they will filter out some candidates with misbehavior: some people lie about their qualification, some posts linked to criminal actions, and discriminative speech against other people. Since misbehaviors would cause serious consequences in the future and lead to more troubles. Meanwhile, even seemingly innocuous details and behaviors, such as using unprofessional terms or publishing too frequently, can also help to judge the drawbacks of the candidates. Moreover, through the combination of algorithms and social media, employers can determine whether the candidate has a strong interest in their company and position. For example, if an applicant browses the company's official website records frequently, looks at position-related articles, or pays attention to the development of the company, the applicants will leave a better impression to the employer. Finally, social media can be a good monitoring station for psychological risks. The employers are able to extract and analyze psychological risks of one candidate from the database of emotional catharsis. For the positive position, there are also some counterclaims that argue against it. One opinion suggested that the company rejects qualified job applicants just because they do not like what they find online. To be more specific, some manger will choose the candidate based on their own preference. Some candidates might lose their just because their race, sexual orientation, or some wrong post they made. Many people thought it is an unfair selection and the surveillance of social media should stop. Also, by looking at the social media webpage of a person, it is easy to form a stereotype of a person while the fact is not like that. For example, if a person posted a picture of drinking, that normally leave bad first impression to the employer. Some employers will subjectively believe that the competitor likes to drink and form the stereotypes of this person. The employers might make the decision of rejection just because he or she thought drink might affect the work in the future. However, the fact would be this competitor just drank alcohol due to a bad mood and did not have the habit of drinking regularly. The way of using social media to measure a person is unfair and also leading to some issues. Thus, employers should not trace the candidates’ personal social media. For this point, I want to make a rebuttal. The behavior of rejecting people just because they post the thing the employer did not like will also influence and bring harmful consequences to the employer’s career. The main mission for the employer is to find a person that really fit certain position and meet the core value of the companies. To find the right candidate, the employer needs to use more comprehensive of measurements (“How to hire the right person”) according to the New York Times. Employers simply use social media as an auxiliary tool to measure competitors. A professionally qualified employer will make an opinion based on the competitor's comprehensive ability and whether he or she is competent for certain specific position. Making decisions based solely on the content of social media is sloppy, and this action will cause the company to lose some potential talent so many employers will not do that. Another aspect we cannot overlook is that social media is still a great tool to reflect the characters of competitors, (Seidman, 2020) and we cannot ignore the importance of this tool. When employees are deciding on two competitors with the same ability, personality may become the main factor determining who will enter the company. In addition, employers are aware of potential legal and regulatory risks. If the hire manager learn that an applicant is a homosexual or other disadvantaged group and hire someone else just because they saw that information on the social media, the manager may face discrimination prosecution. Employers will not take huge risks to make decisions based solely on their personal preferences for social media. As a result, social media can be used as an objective and rational tool to measure the competence of competitors. In general, this problem is still very controversial and involved with many ethnical and technological issues. From all the points that I listed above, I agree with the point that employers should be allowed to surveil their employees' or potential employee’s digital actions and made decisions. Works Cited: How to hire the right person. (n.d.). Retrieved February 22, 2021, from https://www.nytimes.com/guides/business/how-to-hire-the-right-person K. (n.d.). Is it appropriate to assess the job applicant’s social network performance? Retrieved February 22, 2021, from http://www.knowledgeatwharton.com.cn/article/3087/ Number of employers using social media to screen candidates has increased 500 percent over the last decade. (n.d.). Retrieved February 22, 2021, from http://press.careerbuilder.com/2016-04-27-Number-of-Employers-Using-Social-Media-to-Screen-Candidates-Has-Increased-500-Percent-over-the-Last-Decade Seidman, G. (2020, September 21). What can we learn about people from their social media? Retrieved February 22, 2021, from https://www.psychologytoday.com/us/blog/close-encounters/202009/what-can-we-learn-about-people-their-social-media#:~:text=The%20content%20on%20social%20media%20predicts%20personality.&amp;text=Bachrach%20and%20colleagues%20found%20they,is%20related%20to%20their%20personality. Sheri B. Cataldo &amp; Howard S. Weisel. (n.d.). Social media Discovery 101 #federal Courts #isdiscoverypermitted? Retrieved February 23, 2021, from https://litigationcommentary.org/2016/2016-july-august/1248-social-media-discovery-101-federal-courts-isdiscoverypermitted Social media Screenings gain in popularity. (n.d.). Retrieved February 22, 2021, from https://www.businessnewsdaily.com/2377-social-media-hiring.html Tankovska, H. (2021, January 28). Number of social media Users 2025. Retrieved February 22, 2021, from https://www.statista.com/statistics/278414/number-of-worldwide-social-network-users/#:~:text=Social%20media%20usage%20is%20one,almost%204.41%20billion%20in%202025. Tianmai Juyuan (Beijing) Education Technology Co., Ltd. and Ma Wei’s first-instance civil judgment. (n.d.). Retrieved February 22, 2021, from http://www.sq142.com/m/view.php?aid=302 ","link":"https://irisdin.github.io/post/da1sPo1x1/"},{"title":"Analyzing the problems and design of Airbnb interfaces","content":" About the Product: Airbnb is a platform that people can rent their own houses to others. And guests can find the commendations like their own house. A lot of people choose Airbnb since it is normally cheaper than a hotel room and there are lots of options for choosing. People can also earn extra money by renting the house out. The platform gets profits by charging the commission from the costumer’s booking. Currently, people can access Airbnb via its website and mobile app. Stakeholder Research: Direct Stakeholders: The people who booked their house would be the direct stakeholder group for this design. In definition, stakeholders are people who have an interest in the company’s affairs. The tenant wants to find a comfortable place to stay or for other purposes. As a result, this group would be the direct stakeholder. In order to make analysis deeply, I used the research method like surveys and review of online discuss. I choose to survey since there are lots of friends around me are using the Airbnb and I can get lots of real user experience by taking the survey. I designed the questionnaire to ask 30 people who have used Airbnb their satisfaction and concerns about this platform. Moreover, I reviewed the online discussion of Airbnb via the platform like Reddit and App-store. I choose this method since it is easy to access. I look at the user’s rating and their reason for rating specifically. From my research, the stakeholders list lots of suggestions. Their primary motivation for tenant was to find a cheap or a big house according to their needs. Their goal is gaining benefits through the platform. The values they want to gain is a positive cycle for both the renters and tenants. However, the design did not fit their primary motivations, goals, and values. The researched reported many problems: the filter cannot help to find a good house effectively, people cannot comment with videos or pictures, and there are language barriers that may confuse the guests. Indirect Stakeholders: The indirect stakeholders would be the neighborhoods of those renters. They are the indirect stakeholders since the majority of houses that used for Airbnb are private houses. Unlike the hotel, majority of the private living will have a group of neighbors. The design of inaccurate map would make the tenant to knock on neighbor house which disturb their lives. Design Critique : The overall user’s experience and their attitude on the interaction design is good according to the survey and online discussion. And we can see people with various abilities are using Airbnb. However, the platform still contains some problems related to the use’s interfaces and interaction design. To be specific, the filter function cannot help the stakeholders(tenant) to find a ideal house effectively. According to the UI interfaces design principles, it fails to achieve the function of clarity. Moreover, after we selected the place and date in the primary page. The next interfaces that pop up will cause confusions for people with its poor design in visibility and feedback. Problem 1 Label: The first problem is related to the product filter. The design page fails certain categories of UI design principles. One primary principle of UI design is making users comfortable to interact with the design. As the picture shown, the advertisement that listed at the top of page will make the tenant feel uncomfortable. Also, the primary motivation for the tenant is to find a house according to their own need and interests. The filter recommended lots of assumed options for guests and people need to scroll down almost three pages to find some crucial factors that the price. The design is unnecessary and waste people’s time. Another principle for UI design is to reduce the cognitive loads for those stakeholders. The designer should reduce the actions required for achieving the tenant’s goal and extract the most useful information on one page. However, there are too-much information on the filter page and the users need to click lots of times to find a suitable place. Moreover, there is also an interaction design problem. The feedback for the price range is low. The tenant cannot get a good view and feedback about the price they selected through the faded region. Problem 2 Label: The second problem is on the interfaces that displayed house information after the stakeholders clicked the destination and data for travel. According to the UI design principle, the page fails to create an interface that palace users in control. Since when we entered a foreign destination, the house information will translate to the local language and there is no place for translation. For tenant that lived in another country, they will feel confuse about the overall page. They did not what the meaning of those words. And the visibility of the bottom filter is low so some people will ignore that function and the only thing they can get is the picture and price. However, the large portion of the image did not indicate so much useful information. People need to click inside to see the detail information which lead to the cognitive reload. Lastly, the page provides some misleading information. The price showed on the page is not the real price for the house. The renter often adds some extra fees, and the tenant would only know until they reached the final payment page. Redesign Proposal: For the redesigned model, I tried to fix the second problem that stated above. To begin with, the original visibility for filter is poor. The redesign added color with clear word hint for the stakeholders to see the function and select a better house according to their own needs. Also, the redesign model helps to create an interface that easy for people to navigate according to the UI design principle. For the language barrier issue, people can use the translation bottom. Besides, the redesign model put three house information on one page. That saves the stakeholder’s time and also helps them to make compare. The stars make the user directly to see others attitude towards the house on the first page which reduce the cognitive load. For the misleading information price, the new model corrects the price into the average price for the final payment each tenant made before. There are still some limitations with this new design. The first page still did not provide efficient information for tenant to choose a good house. One possible solution could be introducing a sorting bottom that can help to sort the house from the highest price to lowest price, etc. Otherwise, there could be some variation of my design idea. The stars can be transformed into the percentage of user satisfaction. Another variation could be changing the three pictures per page into two pictures per page and add more information about the house. Idea 1 - Redesign of Issue: As we can see on the graph, the redesign of the models involved some changes. The new model includes increasing visibility for the function “filter”, the new translation button, and the stars. Also, the new model put more pictures of houses per page and fix the issue of price. The change of filter is based on the principles of interaction design. Based on Don Norman’s idea, “the more visible the element is, the more likely users will know about them and use them”. Filter is a powerful function that can help people to find an ideal house based on their needs. All other change is based on the principles of UI design, all those changes are devoted to make a user-based interfaces that easy and comfortable to navigate. References: Airbnb (n.d.). Community Center. Retrieved February 01, 2021, from https://community.withairbnb.com/t5/Community-Center/ct-p/community-center (n.d.). Retrieved February 01, 2021, from https://uxdesign.cc/airbnb-redesigning-for-the-new-normal-66fb273de769 Rekhi, S. (2018, February 25). Don Norman's Principles of Interaction Design. Retrieved February 01, 2021, from https://medium.com/@sachinrekhi/don-normans-principles-of-interaction-design-51025a2c0f33 The Basic Principles of User Interface Design. (2020, November 27). Retrieved February 01, 2021, from https://www.uxpin.com/studio/blog/ui-design-principles/ Appendix: ","link":"https://irisdin.github.io/post/59JrCTeTC78/"},{"title":"Logistic Regression Model in U.S. College Admission Result Forecast","content":" Abstract: Logistic Regression is a type of classification algorithm involving a linear discriminant. Logistic Regression model measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logit/sigmoid function. Unlike linear regression, logistic regression does not try to predict the value of a numeric variable given a set of inputs. Instead, the output is a probability that the given input point belongs to a certain class. With the help of the logistic regression model, in this paper, we will create a program to predict the admission decision for U.S. graduate school of Chinese applicant. Input And Output Data Information: Data Processing: When we collect data, outliers may appear. Unlike support vector machine (SVM), Logistic Regression result will be heavily impacted by the outliers (as all data points contribute). If any independent variable in the regression model involves the wrong data, the probability can change hugely. For example, we dropped all GRE total score lower than 250 and Verbal score lower than 120 since they are unlikely to happen in reality and probably out of data entry error Outlier Detection &amp; Processing: Null/Missing Value Imputi Turning Categorical Variables into Numerical Value/One-hot Encod Dealing with Specific Issues in the Datas Materials and Methodology: Libraries like pandas, numpy and pyplot are quite standard python data processing/plottinglibraries that we use for basic data manipulation. Additionally, for this model, we importedthe sklearn library, which is a powerful package that contains our main algorithm of logisticregression. Also, packages like preprocessing and metrics in sklearn can help us preprocessdata and evaluate the performance of the model. For instance, the confusion matrix and itsderived metrics would be used to visualize and evaluate the model performance. Outlier Detection &amp; Processing: SMOTE to Handle Data Imbalance Recursive Feature Elimination (RFE) Model Summary Table &amp; Explanation Results and performance: Link to the paper/ code / dataset: ⬇️ Github ","link":"https://irisdin.github.io/post/projectlogistic/"},{"title":"Informatics major application share :)","content":" Basic information:😀 Cumulative GPA: 3.88 Pre-requsite GPA: 3.90 Course Taken:📝 CSE 160 CSE 180 INFO 200 STAT 311 ... Info related experience/skills 🔧: 🤘Coding：python, java, R 🕸️web development：html5, Javascript 👩‍🎨Visual design：Autocad, Figma, Balsamiq 👩‍💻Data analysis：PowerBI，Tableau，Microsoft Excel ✍️Editor：Vscode ,Eclipse You could check my projects on: Linkedin Github Application essay: 🌟Major component: Experiences with information Experiences with diversity, equity, and inclusion. Goals after college. Learning skills. Writing My decision to study informatics derives from my ambition to serve human beings through technology, but coming to that decision was a process of exploration. I was first exposed to contents related to informatics in My senior year in high school. In the beginning of Covid-19 in 2020, all of us was panic due to lack of information about the virus. I searched all news and media to put the pieces together and caught the sight of whole puzzle. That was when I realized the importance of information. An efficient information system could help me turn information into actionable knowledge. Therefore, I have a strong intent to develop solutions to the world's information challenges for myself and the communities. This is my first love into informatics. After that, in the university, I have a deeper understanding of informatics. As a member of Chinese students who account for the vast majority of American international students, I’m much better able to realize that international students are facing many serious social, physical and spiritual problems. Especially during the pandemic period, many students can only take online classes, so time equation, lacking of communication with teachers and other questions cause emotional problems and academic pressure. In communicating with classmates, I found that many are under this kind of pressure silently, some even suspend or drop out, including myself. Furthermore, I found that not only Chinese students, but students from other countries also face the same psychological problems. Thus, I had the idea of designing an app to help solve the psychological problems of students. I would like to design a platform to help this special group in our community. By using machine learning methods, I could collect data，provide them information from the school, different solutions for their concerns and match different friends or mentors to help them. So here comes Mental Bridge. Our team for Info200 build the simple framework of the app together. But that was not enough, I would like to make the platform and help our international community in real world. Through three semesters' university life, I am more determined in learning informatics. The past learning experience has given me enough confidence and ability to deal with future tasks. I took a glimpse of the whole information world though Info200. I learned fundamental skills of manipulating data, machine learning and making simple regression in Info180. By using right ways to analyze data through the models in this world which has tons of information, I could push right news and right people to help my Mental Bridge users. Therefore, interest and reality motivated me to make the determination to study informatics. In my opinion, diversified courses in major informatics will make me be able to learn foundational skills and practical technology in information technology, also provide me with an interdisciplinary perspective to close the gap between technology and human needs by translating users’ requirements into technological solutions. The course Data Reasoning in a Digital World can teach me analyze the authenticity of information sources, on the basis of true information. The course Foundational Skills for Data Science can teach me how to analyze and use data through programming. While Databases and Data Modeling introduces the relevant knowledge of the database system, and links data modeling decisions to social justice outcomes. What’s more, my serious learning attitude and hardworking spirit will also help me to my future study of informatics. I believe that after learning more knowledge, I can continuously optimize and develop Mental Bridge, so that it can collect students’ psychological problems and provide suitable solutions. In other words, I want to finally create a bridge connecting students and counselors to solve the current situation when seeking help. And through my study in Informatics, I could make the program more accurate and eliminate the bias of misunderstanding users' needs in the progress of matching right solutions for every single student who needs help not only Chinese students. We are living in a era of major technological advances and information exploration. I will dedicate myself in studying how to use these information to improve our lives and make our community better. Through informatics major, I will explore the methods of change our community or even our world. all right reserved/any form of plagiarism is not allowed ","link":"https://irisdin.github.io/post/info/"},{"title":"About","content":" weclcome to my website:) 🏠 About the site This is a personal website that generate my work, thought and project. * remember to click to read the full content 👨‍💻 About me ISFJ-A From Ningbo China➡️Seattle US The barstow school 20' University of washington informatics 24' : data science &amp; human computer interacton track A code writer/data science Enthusiast/ future educator :) 📬 contact information email address: &lt;irisding20020213@outlook.com&gt; follow me on: instagram: @iris_oovo feel free to contatct me if you have any questions. ","link":"https://irisdin.github.io/post/about/"}]}